{"nbformat":4,"nbformat_minor":5,"metadata":{"accelerator":"GPU","colab":{"name":"retailer_baseline.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"widgets":{"application/vnd.jupyter.widget-state+json":{"61f34a130734463c85e8123bc1848923":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b9f51da54b0f4685a537607facee6f25","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e1da1450d9734ce3bd6f427e425d5758","IPY_MODEL_2836541792cf4ed992b7bc88acf81d36","IPY_MODEL_4cc045003c73403c93cebef0b046d88c"]}},"b9f51da54b0f4685a537607facee6f25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e1da1450d9734ce3bd6f427e425d5758":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d384fc8437a7411b90eb9322034afaa1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b70dc111efc44e27867bea7779d23c70"}},"2836541792cf4ed992b7bc88acf81d36":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_43a1184d48bf486a9f314d8f6b4d7212","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":288,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":288,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_eb299b519ead4c35a3ec1033ea58c7fd"}},"4cc045003c73403c93cebef0b046d88c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_95ce9f3766c34cf0bc7228f8578cc74d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 288/288 [00:00&lt;00:00, 11.7kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_31779c0a7526433183fa3221c92cc06b"}},"d384fc8437a7411b90eb9322034afaa1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b70dc111efc44e27867bea7779d23c70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"43a1184d48bf486a9f314d8f6b4d7212":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"eb299b519ead4c35a3ec1033ea58c7fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"95ce9f3766c34cf0bc7228f8578cc74d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"31779c0a7526433183fa3221c92cc06b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6932f63c8bd54c03b8734143b9e969c9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_fb4f1397cbac4ebcba11ab58fdeb7776","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8e3919cd1bd34986b427817d66cf4b37","IPY_MODEL_1bedcea0e5274063997a1aab9b723865","IPY_MODEL_44702fb06d734c26b7a2e61293ec507e"]}},"fb4f1397cbac4ebcba11ab58fdeb7776":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8e3919cd1bd34986b427817d66cf4b37":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_684960dd977d4d07a753c6a7695bf38f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_491ddbb25b2547cf902671a74710ceb2"}},"1bedcea0e5274063997a1aab9b723865":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_88e2210887c243e9a4fa73068834b259","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":504,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":504,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cbd3dbd91a4f4d8fb5f77a9ab8d9caca"}},"44702fb06d734c26b7a2e61293ec507e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ff2a99cc1165418ebac97876f2896047","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 504/504 [00:00&lt;00:00, 19.8kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_42266290e2884a308360e330eb4983d8"}},"684960dd977d4d07a753c6a7695bf38f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"491ddbb25b2547cf902671a74710ceb2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"88e2210887c243e9a4fa73068834b259":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"cbd3dbd91a4f4d8fb5f77a9ab8d9caca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ff2a99cc1165418ebac97876f2896047":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"42266290e2884a308360e330eb4983d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"208a30515ad04c51b4bb7967850e21a6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bd3623091dc34626839ce4b0a3e924ff","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5d31c79020f94be3b471dd9daef45f54","IPY_MODEL_68cb5fb9676f4b9ab14fd6c71be9f546","IPY_MODEL_9d9031962a84453aa4c89fe9c029322a"]}},"bd3623091dc34626839ce4b0a3e924ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5d31c79020f94be3b471dd9daef45f54":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_27279c65b0a848288af88fcc7010e3c4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a1d30b8e36ea4db793712e3acbdc9aaa"}},"68cb5fb9676f4b9ab14fd6c71be9f546":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_171931e005e4435fa07858426ebafa80","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":396417,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":396417,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5555882946f44e55af190e6410b33425"}},"9d9031962a84453aa4c89fe9c029322a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5dafbc2cf84c45b0ac2de424e4625188","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 396k/396k [00:00&lt;00:00, 1.21MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6f307cbc206e41d3a641c773e597f120"}},"27279c65b0a848288af88fcc7010e3c4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a1d30b8e36ea4db793712e3acbdc9aaa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"171931e005e4435fa07858426ebafa80":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5555882946f44e55af190e6410b33425":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5dafbc2cf84c45b0ac2de424e4625188":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6f307cbc206e41d3a641c773e597f120":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"de86c3e3fe254371930a503af2a12585":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_be8d36cf2cd44b25b71d23514af23ba1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8ae2237389624c949094c08ea7ea8c02","IPY_MODEL_fab1d86c7a644af0934ceff86d36a1ab","IPY_MODEL_ffb2c468301e456dacf6bfd09f25fc29"]}},"be8d36cf2cd44b25b71d23514af23ba1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8ae2237389624c949094c08ea7ea8c02":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_387d18bc78c54000948d32ea7dc8be7a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3bbd5a5d06b240fd9c1785df29a00114"}},"fab1d86c7a644af0934ceff86d36a1ab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4e27e49b6ff04bdd87841b56cc7bdf6b","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":124,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":124,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3c092b8d9bde4993b85cdef58447316c"}},"ffb2c468301e456dacf6bfd09f25fc29":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b4e825aa41e94800aea2e96637592d7f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 124/124 [00:00&lt;00:00, 5.13kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a9b8b3f7a2e84cc6bf81da2e12177520"}},"387d18bc78c54000948d32ea7dc8be7a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3bbd5a5d06b240fd9c1785df29a00114":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4e27e49b6ff04bdd87841b56cc7bdf6b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3c092b8d9bde4993b85cdef58447316c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b4e825aa41e94800aea2e96637592d7f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a9b8b3f7a2e84cc6bf81da2e12177520":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"afc228ab11ad49d39dbe51cfcd2ef117":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_07ff7aa4466049ed8c91c57e04ea49bc","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_755eaf22fd5748b1a0a659c9eed36813","IPY_MODEL_5323831ef982448ab450282da9669ecf","IPY_MODEL_f7187e5aabcb41b0b833439b996ba814"]}},"07ff7aa4466049ed8c91c57e04ea49bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"755eaf22fd5748b1a0a659c9eed36813":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ab19d08ae72b42eeb11b1c50e26b8bc3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_327db6a5c4984109b5eaf50d2c0d4420"}},"5323831ef982448ab450282da9669ecf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1fbb90ff64064fe6ad0d745f974ccc70","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":498271049,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":498271049,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f9e5884d2abf49759711f9f52d005999"}},"f7187e5aabcb41b0b833439b996ba814":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_cf03ac82702f4e07845d83b0970a1c21","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 498M/498M [00:08&lt;00:00, 63.8MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d65c9f79c1db44a688c4a0a32f61d09f"}},"ab19d08ae72b42eeb11b1c50e26b8bc3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"327db6a5c4984109b5eaf50d2c0d4420":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1fbb90ff64064fe6ad0d745f974ccc70":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f9e5884d2abf49759711f9f52d005999":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cf03ac82702f4e07845d83b0970a1c21":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d65c9f79c1db44a688c4a0a32f61d09f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"_sAfcF8H5s9f"},"source":["# 소상공인 QnA 카테고리 분류"],"id":"_sAfcF8H5s9f"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"af2fe426-f127-43ff-b25a-390162868999","executionInfo":{"status":"ok","timestamp":1630139351726,"user_tz":-540,"elapsed":18066,"user":{"displayName":"한주형","photoUrl":"","userId":"08880882316688407325"}},"outputId":"3321a3bf-3344-4697-ee77-5099bacba284"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"af2fe426-f127-43ff-b25a-390162868999","execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JRp8UqXn0jtx","executionInfo":{"status":"ok","timestamp":1630139351727,"user_tz":-540,"elapsed":5,"user":{"displayName":"한주형","photoUrl":"","userId":"08880882316688407325"}}},"source":["!ln -s /content/drive/MyDrive/ /gdrive"],"id":"JRp8UqXn0jtx","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"JPUMc4pG0jrL","executionInfo":{"status":"ok","timestamp":1630139351727,"user_tz":-540,"elapsed":4,"user":{"displayName":"한주형","photoUrl":"","userId":"08880882316688407325"}}},"source":["!mkdir data/"],"id":"JPUMc4pG0jrL","execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"zddzsuaB0joO","executionInfo":{"status":"ok","timestamp":1630139372956,"user_tz":-540,"elapsed":21232,"user":{"displayName":"한주형","photoUrl":"","userId":"08880882316688407325"}}},"source":["!cp -r /gdrive/aicon/aicon_retrailer/train data/\n","!cp -r /gdrive/aicon/aicon_retrailer/val data/\n","!cp -r /gdrive/aicon/aicon_retrailer/test data/\n","!cp /gdrive/aicon/aicon_retrailer/sample_submission.csv data/"],"id":"zddzsuaB0joO","execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"d1f46b40-8884-40ac-829b-e2ba1c8f384c","executionInfo":{"status":"ok","timestamp":1630139375628,"user_tz":-540,"elapsed":2679,"user":{"displayName":"한주형","photoUrl":"","userId":"08880882316688407325"}}},"source":["import logging\n","\n","import os\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from datetime import datetime, timezone, timedelta\n","import numpy as np\n","import torch\n","import torch.optim as optim\n","import random\n","from sklearn.metrics import accuracy_score\n","from torch.utils.data import DataLoader"],"id":"d1f46b40-8884-40ac-829b-e2ba1c8f384c","execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"466e6fcf-99b5-4ede-86ad-e08496c20d9d","executionInfo":{"status":"ok","timestamp":1630139375628,"user_tz":-540,"elapsed":7,"user":{"displayName":"한주형","photoUrl":"","userId":"08880882316688407325"}}},"source":["logging.disable(logging.WARNING)\n","\n","# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"id":"466e6fcf-99b5-4ede-86ad-e08496c20d9d","execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"075d7e81-a656-43e9-b68c-2a60e5c30aba","executionInfo":{"status":"ok","timestamp":1630139375628,"user_tz":-540,"elapsed":6,"user":{"displayName":"한주형","photoUrl":"","userId":"08880882316688407325"}}},"source":["# 시드(seed) 설정\n","\n","RANDOM_SEED = 2021\n","torch.manual_seed(RANDOM_SEED)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","np.random.seed(RANDOM_SEED)\n","random.seed(RANDOM_SEED)"],"id":"075d7e81-a656-43e9-b68c-2a60e5c30aba","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"b927e602-564c-44af-bf7d-13875337ccf3","executionInfo":{"status":"ok","timestamp":1630139375629,"user_tz":-540,"elapsed":6,"user":{"displayName":"한주형","photoUrl":"","userId":"08880882316688407325"}}},"source":["# working directory 지정\n","ROOT_PATH = './data'\n","DATA_DIR = os.path.join(ROOT_PATH)\n","RESULT_DIR = os.path.join(ROOT_PATH, 'result')\n","if not os.path.isdir(RESULT_DIR):\n","  os.makedirs(RESULT_DIR)\n","\n","# hyper-parameters\n","EPOCHS = 30\n","BATCH_SIZE = 8\n","LEARNING_RATE = 0.0005\n","EARLY_STOPPING_PATIENCE = 5"],"id":"b927e602-564c-44af-bf7d-13875337ccf3","execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"c3f48a43-3f04-4edf-8bb2-fd6c9c59e775","executionInfo":{"status":"ok","timestamp":1630139375629,"user_tz":-540,"elapsed":5,"user":{"displayName":"한주형","photoUrl":"","userId":"08880882316688407325"}}},"source":["def get_logger(name: str, file_path: str, stream=False) -> logging.RootLogger:\n","    logger = logging.getLogger(name)\n","    logger.setLevel(logging.INFO)\n","\n","    formatter = logging.Formatter('%(asctime)s | %(name)s | %(levelname)s | %(message)s')\n","    stream_handler = logging.StreamHandler()\n","    file_handler = logging.FileHandler(file_path)\n","\n","    stream_handler.setFormatter(formatter)\n","    file_handler.setFormatter(formatter)\n","\n","    if stream:\n","        logger.addHandler(stream_handler)\n","    logger.addHandler(file_handler)\n","\n","    return logger\n","\n","# Set system logger\n","system_logger = get_logger(name='train',file_path=os.path.join(ROOT_PATH,'/02_retailer_train_log.log'))"],"id":"c3f48a43-3f04-4edf-8bb2-fd6c9c59e775","execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d7d1b87b-fa8c-4c0d-b91d-63e411f29ff3"},"source":["## Dataloader"],"id":"d7d1b87b-fa8c-4c0d-b91d-63e411f29ff3"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HEAGPeEGUcYo","executionInfo":{"status":"ok","timestamp":1630139382174,"user_tz":-540,"elapsed":6549,"user":{"displayName":"한주형","photoUrl":"","userId":"08880882316688407325"}},"outputId":"ac59b41f-e33a-4f2f-d997-13f2a7cdc2a6"},"source":["!pip install transformers"],"id":"HEAGPeEGUcYo","execution_count":10,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n","\u001b[K     |████████████████████████████████| 2.6 MB 8.3 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 62.9 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 70.2 MB/s \n","\u001b[?25hCollecting huggingface-hub==0.0.12\n","  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 50.7 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"a47b3e0a-63b2-49dc-a287-58de25c6174c","executionInfo":{"status":"ok","timestamp":1630139384345,"user_tz":-540,"elapsed":2178,"user":{"displayName":"한주형","photoUrl":"","userId":"08880882316688407325"}}},"source":["from torch.utils.data import Dataset\n","from transformers import AutoTokenizer\n","from itertools import chain\n","import sys\n","\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, data_dir, mode):\n","        self.mode = mode\n","        self.data_dir = data_dir\n","        self.intents = {'AS_날짜_요청': 0,'AS_날짜_질문': 1,'AS_방법_요청': 2,'AS_방법_질문': 3,'AS_비용_요청': 4,'AS_비용_질문': 5,\n","                        'AS_시간_질문': 6,'AS_일반_질문': 7,'결제_방식_질문': 8,'결제_수단_질문': 9,'결제_시기_질문': 10,'결제_영수증_질문': 11,\n","                        '결제_오류_질문': 12,'결제_일반_질문': 13,'결제_일반_확인': 14,'결제_재결제_질문': 15,'결제_추가_질문': 16,\n","                        '결제_취소_질문': 17,'결제_할인_질문': 18,'교환|반품|환불_방법_요청': 19,'교환|반품|환불_방법_질문': 20,\n","                        '교환|반품|환불_방법_확인': 21,'교환|반품|환불_비용_질문': 22,'교환|반품|환불_시간_요청': 23,\n","                        '교환|반품|환불_시간_질문': 24,'교환|반품|환불_일반_요청': 25,'교환|반품|환불_일반_질문': 26,\n","                        '교환|반품|환불_일반_확인': 27,'구매_예약_요청': 28,'구매_예약_질문': 29,'구매_제품_요청': 30,'구매_제품_질문': 31,\n","                        '매장_이용_요청': 32,'매장_이용_질문': 33,'매장_정보_질문': 34,'멤버십_사용_질문': 35,'멤버십_적립_질문': 36,\n","                        '배송_날짜_요청': 37,'배송_날짜_질문': 38,'배송_날짜_확인': 39,'배송_방법_요청': 40,'배송_방법_질문': 41,\n","                        '배송_방법_확인': 42,'배송_비용_질문': 43,'배송_오류_질문': 44,'배송_오류_확인': 45,'배송_일반_요청': 46,\n","                        '배송_일반_질문': 47,'배송_일반_확인': 48,'배송_지역_요청': 49,'배송_지역_질문': 50,'배송_택배사_질문': 51,\n","                        '부가서비스_날짜_요청': 52,'부가서비스_날짜_질문': 53,'부가서비스_방법_요청': 54,'부가서비스_방법_질문': 55,\n","                        '부가서비스_비용_요청': 56,'부가서비스_비용_질문': 57,'웹사이트_사용_질문': 58,'웹사이트_오류_질문': 59,\n","                        '제품_가격_비교': 60,'제품_가격_요청': 61,'제품_가격_질문': 62,'제품_가격_확인': 63,'제품_구성_요청': 64,\n","                        '제품_구성_질문': 65,'제품_구성_확인': 66,'제품_날짜_질문': 67,'제품_방법_요청': 68,'제품_방법_질문': 69,\n","                        '제품_방법_확인': 70,'제품_불량_요청': 71,'제품_불량_질문': 72,'제품_불량_확인': 73,'제품_소재_질문': 74,\n","                        '제품_시용_요청': 75,'제품_시용_질문': 76,'제품_용도_질문': 77,'제품_용도_확인': 78,'제품_원산지_질문': 79,\n","                        '제품_일반_비교': 80,'제품_일반_요청': 81,'제품_일반_질문': 82,'제품_일반_확인': 83,'제품_입고_요청': 84,\n","                        '제품_입고_질문': 85,'제품_재고_요청': 86,'제품_재고_질문': 87,'제품_재고_확인': 88,'제품_정보_비교': 89,\n","                        '제품_정보_요청': 90,'제품_정보_질문': 91,'제품_정보_확인': 92,'제품_추천_비교': 93,'제품_추천_요청': 94,\n","                        '제품_추천_질문': 95,'제품_추천_확인': 96,'제품_커스텀_요청': 97,'제품_커스텀_질문': 98,'제품_품질_비교': 99,\n","                        '제품_품질_요청': 100,'제품_품질_질문': 101,'제품_품질_확인': 102,'제품_호환_질문': 103,'제품_호환_확인': 104,\n","                        '포장_방식_요청': 105,'포장_방식_질문': 106,'포장_비용_질문': 107,'포장_일반_질문': 108,'행사_기간_질문': 109,\n","                        '행사_기간_확인': 110,'행사_날짜_질문': 111,'행사_유형_질문': 112,'행사_유형_확인': 113,'행사_일반_질문': 114,\n","                        '행사_일반_확인': 115,'행사_정보_요청': 116,'행사_정보_질문': 117}\n","        self.num_labels = len(self.intents)\n","        self.tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")\n","        # Load data\n","        self.inputs, self.labels = self.data_loader(data_dir)\n","\n","\n","    def data_loader(self, path):\n","        print('Loading ' + self.mode + ' dataset..')\n","        # check if preprocessed data directory exists\n","        if not os.path.isdir(self.data_dir):\n","            print(f'!!! Cannot find {self.data_dir}... !!!')\n","            sys.exit()\n","\n","        if os.path.isfile(os.path.join(path, self.mode, self.mode + '_X.pt')):\n","            inputs = torch.load(os.path.join(path, self.mode, self.mode + '_X.pt'))\n","            labels = torch.load(os.path.join(path, self.mode, self.mode + '_Y.pt'))\n","\n","        else:\n","            file_path = os.path.join(path, self.mode, self.mode + '.csv')\n","            df = pd.read_csv(file_path)\n","            df = df.dropna(axis=0, how='all')\n","            inputs = df[df.columns[2:]]\n","            labels = df['intent']\n","\n","            # Preprocessing\n","            inputs, labels = self.preprocessing(inputs, labels)\n","            # Save data\n","            torch.save(inputs ,os.path.join(path, self.mode, self.mode + '_X.pt'))\n","            torch.save(labels, os.path.join(path, self.mode, self.mode + '_Y.pt'))\n","\n","        return inputs, labels\n","\n","    def pad(self, data, pad_id, max_len):\n","        padded_data = list(map(lambda x : torch.cat([x, torch.tensor([pad_id] * (max_len - len(x)))]), data))\n","        return padded_data\n","\n","    def preprocessing(self, inputs, labels):\n","        print('Preprocessing ' + self.mode + ' dataset..')\n","        #Encoding original\n","        src_tensor = []\n","        seg_tensor = []\n","        for i in range(len(inputs)):\n","            src_tensor.append(torch.tensor(list(chain.from_iterable([self.tokenizer.encode(inputs[col][i], add_special_tokens=True) \\\n","                                                                     for col in inputs.columns if inputs[col][i] == inputs[col][i]]))))\n","            clss = torch.cat([torch.where(src_tensor[i] == 2)[0], torch.tensor([len(src_tensor[i])])])\n","            seg_tensor.append(torch.tensor(list(chain.from_iterable( \\\n","                [[0] * (clss[i + 1] - clss[i]) if i % 2 == 0 else [1] * (clss[i + 1] - clss[i]) \\\n","                for i, val in enumerate(clss[:-1])]))))\n","\n","        #Padding\n","        max_encoding_len = max(list(map(lambda x: len(x), src_tensor)))\n","        assert max_encoding_len < 512, 'Encoding length is longer than maximum processing length.'\n","        src_tensor = self.pad(src_tensor, 0, max_encoding_len)\n","        seg_tensor = self.pad(seg_tensor, 0, max_encoding_len)\n","\n","        #Convert to list of tensor to 2d tensor\n","        src_tensor = torch.stack(src_tensor, dim=0)\n","        seg_tensor = torch.stack(seg_tensor, dim=0)\n","        mask_tensor = (~ (src_tensor == 0))\n","\n","        #Encoding labels\n","        label_tensor = torch.tensor(self.label_encoder(labels.values))\n","\n","\n","        #Integrate the tensor {1st dimension : {src, seg, mask}, 2nd dim : {number of samples}, 3rd dim : {encoding dimension}}\n","        input_tensor = torch.cat([src_tensor.unsqueeze(dim=1) , seg_tensor.unsqueeze(dim=1), mask_tensor.unsqueeze(dim=1)], dim=1)\n","\n","        return input_tensor, label_tensor\n","\n","    def label_encoder(self, labels):\n","        try:\n","            labels = list(map(lambda x : self.intents[x], labels))\n","            return labels\n","        except:\n","            assert 'Invalid intent'\n","\n","    def __len__(self):\n","        return len(self.inputs)\n","\n","    def __getitem__(self, index):\n","        return self.inputs[index, :, :], self.labels[index]\n"],"id":"a47b3e0a-63b2-49dc-a287-58de25c6174c","execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"037b3e44-feb8-4804-93b9-b126cab64085","colab":{"base_uri":"https://localhost:8080/","height":180,"referenced_widgets":["61f34a130734463c85e8123bc1848923","b9f51da54b0f4685a537607facee6f25","e1da1450d9734ce3bd6f427e425d5758","2836541792cf4ed992b7bc88acf81d36","4cc045003c73403c93cebef0b046d88c","d384fc8437a7411b90eb9322034afaa1","b70dc111efc44e27867bea7779d23c70","43a1184d48bf486a9f314d8f6b4d7212","eb299b519ead4c35a3ec1033ea58c7fd","95ce9f3766c34cf0bc7228f8578cc74d","31779c0a7526433183fa3221c92cc06b","6932f63c8bd54c03b8734143b9e969c9","fb4f1397cbac4ebcba11ab58fdeb7776","8e3919cd1bd34986b427817d66cf4b37","1bedcea0e5274063997a1aab9b723865","44702fb06d734c26b7a2e61293ec507e","684960dd977d4d07a753c6a7695bf38f","491ddbb25b2547cf902671a74710ceb2","88e2210887c243e9a4fa73068834b259","cbd3dbd91a4f4d8fb5f77a9ab8d9caca","ff2a99cc1165418ebac97876f2896047","42266290e2884a308360e330eb4983d8","208a30515ad04c51b4bb7967850e21a6","bd3623091dc34626839ce4b0a3e924ff","5d31c79020f94be3b471dd9daef45f54","68cb5fb9676f4b9ab14fd6c71be9f546","9d9031962a84453aa4c89fe9c029322a","27279c65b0a848288af88fcc7010e3c4","a1d30b8e36ea4db793712e3acbdc9aaa","171931e005e4435fa07858426ebafa80","5555882946f44e55af190e6410b33425","5dafbc2cf84c45b0ac2de424e4625188","6f307cbc206e41d3a641c773e597f120","de86c3e3fe254371930a503af2a12585","be8d36cf2cd44b25b71d23514af23ba1","8ae2237389624c949094c08ea7ea8c02","fab1d86c7a644af0934ceff86d36a1ab","ffb2c468301e456dacf6bfd09f25fc29","387d18bc78c54000948d32ea7dc8be7a","3bbd5a5d06b240fd9c1785df29a00114","4e27e49b6ff04bdd87841b56cc7bdf6b","3c092b8d9bde4993b85cdef58447316c","b4e825aa41e94800aea2e96637592d7f","a9b8b3f7a2e84cc6bf81da2e12177520"]},"executionInfo":{"status":"ok","timestamp":1630139391562,"user_tz":-540,"elapsed":7221,"user":{"displayName":"한주형","photoUrl":"","userId":"08880882316688407325"}},"outputId":"f77de0fd-a5c7-492f-f8ce-1f62eb56bd37"},"source":["train_dataset = CustomDataset(data_dir=DATA_DIR, mode='train')\n","validation_dataset = CustomDataset(data_dir=DATA_DIR, mode='val')\n","train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","validation_dataloader = DataLoader(dataset=validation_dataset, batch_size=BATCH_SIZE, shuffle=False)"],"id":"037b3e44-feb8-4804-93b9-b126cab64085","execution_count":12,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"61f34a130734463c85e8123bc1848923","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/288 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6932f63c8bd54c03b8734143b9e969c9","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/504 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"208a30515ad04c51b4bb7967850e21a6","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/396k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"de86c3e3fe254371930a503af2a12585","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/124 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","text":["Loading train dataset..\n","Loading val dataset..\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1298fcef-bedf-48dd-add4-a2611894497b"},"source":["## Model"],"id":"1298fcef-bedf-48dd-add4-a2611894497b"},{"cell_type":"code","metadata":{"id":"f7e19863-26e1-4272-b010-11bb35b0ca65","colab":{"base_uri":"https://localhost:8080/","height":782},"executionInfo":{"status":"ok","timestamp":1630139397578,"user_tz":-540,"elapsed":6024,"user":{"displayName":"한주형","photoUrl":"","userId":"08880882316688407325"}},"outputId":"87931dd3-a455-4274-e885-e9284070e368"},"source":["!pip install pytorch-pretrained-bert"],"id":"f7e19863-26e1-4272-b010-11bb35b0ca65","execution_count":13,"outputs":[{"output_type":"stream","text":["Collecting pytorch-pretrained-bert\n","  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n","\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 36.6 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 26.0 MB/s eta 0:00:01\r\u001b[K     |████████                        | 30 kB 18.4 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40 kB 16.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 123 kB 7.2 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.62.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.19.5)\n","Collecting boto3\n","  Downloading boto3-1.18.31-py3-none-any.whl (131 kB)\n","\u001b[K     |████████████████████████████████| 131 kB 85.1 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.9.0+cu102)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n","Collecting botocore<1.22.0,>=1.21.31\n","  Downloading botocore-1.21.31-py3-none-any.whl (7.8 MB)\n","\u001b[K     |████████████████████████████████| 7.8 MB 62.6 MB/s \n","\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n","  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 10.7 MB/s \n","\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n","  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.31->boto3->pytorch-pretrained-bert) (2.8.2)\n","Collecting urllib3<1.27,>=1.25.4\n","  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n","\u001b[K     |████████████████████████████████| 138 kB 81.4 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.31->boto3->pytorch-pretrained-bert) (1.15.0)\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 78.9 MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed boto3-1.18.31 botocore-1.21.31 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.5.0 urllib3-1.25.11\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["urllib3"]}}},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"b8bbbe6f-68a5-4a29-b1f7-6d6173972030","executionInfo":{"status":"ok","timestamp":1630139397578,"user_tz":-540,"elapsed":6,"user":{"displayName":"한주형","photoUrl":"","userId":"08880882316688407325"}}},"source":["from pytorch_pretrained_bert import BertModel\n","import pytorch_pretrained_bert\n","import transformers\n","\n","class IntentClassifier(nn.Module):\n","\n","    def __init__(self):\n","        \"\"\"\n","        \"\"\"\n","        super(IntentClassifier, self).__init__()\n","        self.model = transformers.BertForSequenceClassification.from_pretrained(\"beomi/KcELECTRA-base\", num_labels = 118)\n","        self.hidden_dim = 768\n","        self.fc = nn.Linear(self.hidden_dim, 6)\n","\n","    def forward(self, x, mask, segs, target=None):\n","        if target is not None:\n","            output = self.model(input_ids = x.long(), attention_mask = mask.float(), token_type_ids = segs.long(), labels=target.unsqueeze(1))\n","        else:\n","            output = self.model(input_ids = x.long(), attention_mask = mask.float(), token_type_ids = segs.long())\n","        return output\n"],"id":"b8bbbe6f-68a5-4a29-b1f7-6d6173972030","execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"a56b4de4-7faf-4c07-ad96-928e1b170b20","executionInfo":{"status":"ok","timestamp":1630139397579,"user_tz":-540,"elapsed":6,"user":{"displayName":"한주형","photoUrl":"","userId":"08880882316688407325"}}},"source":["class LossEarlyStopper():\n","    \"\"\"Early stopper\n","    \n","    Attributes:\n","        patience (int): loss가 줄어들지 않아도 학습할 epoch 수\n","        verbose (bool): 로그 출력 여부, True 일 때 로그 출력\n","        patience_counter (int): loss 가 줄어들지 않을 때 마다 1씩 증가\n","        min_loss (float): 최소 loss\n","        stop (bool): True 일 때 학습 중단\n","\n","    \"\"\"\n","\n","    def __init__(self, patience: int, verbose: bool, logger:logging.RootLogger=None)-> None:\n","        \"\"\" 초기화\n","\n","        Args:\n","            patience (int): loss가 줄어들지 않아도 학습할 epoch 수\n","            weight_path (str): weight 저장경로\n","            verbose (bool): 로그 출력 여부, True 일 때 로그 출력\n","        \"\"\"\n","        self.patience = patience\n","        self.verbose = verbose\n","\n","        self.patience_counter = 0\n","        self.min_loss = np.Inf\n","        self.logger = logger\n","        self.stop = False\n","\n","    def check_early_stopping(self, loss: float)-> None:\n","        \"\"\"Early stopping 여부 판단\n","\n","        Args:\n","            loss (float):\n","\n","        Examples:\n","            \n","        Note:\n","            \n","        \"\"\"  \n","\n","        if self.min_loss == np.Inf:\n","            self.min_loss = loss\n","            # self.save_checkpoint(loss=loss, model=model)\n","\n","        elif loss > self.min_loss:\n","            self.patience_counter += 1\n","            msg = f\"Early stopper, Early stopping counter {self.patience_counter}/{self.patience}\"\n","\n","            if self.patience_counter == self.patience:\n","                self.stop = True\n","\n","            if self.verbose:\n","                self.logger.info(msg) if self.logger else print(msg)\n","                \n","        elif loss <= self.min_loss:\n","            self.save_model = True\n","            msg = f\"Early stopper, Validation loss decreased {self.min_loss} -> {loss}\"\n","            self.min_loss = loss\n","            # self.save_checkpoint(loss=loss, model=model)\n","\n","            if self.verbose:\n","                self.logger.info(msg) if self.logger else print(msg)"],"id":"a56b4de4-7faf-4c07-ad96-928e1b170b20","execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"52ee23ce-a7de-447e-b4a4-eb892715ecd7","executionInfo":{"status":"ok","timestamp":1630139397579,"user_tz":-540,"elapsed":6,"user":{"displayName":"한주형","photoUrl":"","userId":"08880882316688407325"}}},"source":["class Trainer():\n","\n","    def __init__(self, model, device, loss_fn, metric_fn, optimizer=None, scheduler=None, logger=None):\n","        \"\"\" 초기화\n","        \"\"\"\n","        self.model = model\n","        self.device = device\n","        self.optimizer = optimizer\n","        self.loss_fn = loss_fn\n","        self.logger = logger\n","        self.scheduler = scheduler\n","        self.metric_fn = metric_fn\n","\n","    def train_epoch(self, dataloader, epoch_index):\n","        \"\"\" 한 epoch에서 수행되는 학습 절차\n","\n","        Args:\n","            dataloader (`dataloader`)\n","            epoch_index (int)\n","        \"\"\"\n","        self.model.train()\n","        self.train_total_loss = 0\n","        target_lst = []\n","        pred_lst = []\n","        for batch_index, (data, target) in enumerate(dataloader):\n","            data = data.to(self.device)\n","            target = target.to(self.device)\n","            src = data[:, 0, :]\n","            segs = data[:, 1, :]\n","            mask = data[:, 2, :]\n","            output = self.model(src, mask, segs, target)\n","            loss = output.loss\n","            self.optimizer.zero_grad()\n","            loss.backward()\n","            self.optimizer.step()\n","            self.scheduler.step()\n","            self.train_total_loss += loss\n","            target_lst.extend(target.cpu().tolist())\n","            pred = output.logits.argmax(dim=1)\n","            pred_lst.extend(pred.cpu().tolist())\n","            \n","            batch_score = self.metric_fn(target_lst, pred_lst)\n","            \n","            msg = f\"Epoch {epoch_index} train batch {batch_index}/{len(dataloader)}: {batch_index * dataloader.batch_size}/{len(dataloader)} mean loss: {loss} score: {batch_score}\"\n","            if batch_index%100 == 0:\n","                if self.logger:\n","                    self.logger.info(msg)\n","                print(msg)\n","            \n","        self.train_mean_loss = self.train_total_loss / len(dataloader)\n","        self.train_score = accuracy_score(y_true=target_lst, y_pred=pred_lst)\n","        msg = f'Epoch {epoch_index}, Train, loss: {self.train_mean_loss}, Score: {self.train_score}'\n","        print(msg)\n","        self.logger.info(msg) if self.logger else print(msg)\n","\n","    def validate_epoch(self, dataloader, epoch_index):\n","        \"\"\" 한 epoch에서 수행되는 검증 절차\n","\n","        Args:\n","            dataloader (`dataloader`)\n","            epoch_index (int)\n","        \"\"\"\n","        self.model.eval()\n","        self.val_total_loss = 0\n","        target_lst = []\n","        pred_lst = []\n","        with torch.no_grad():\n","            for batch_index, (data, target) in enumerate(dataloader):\n","                data = data.to(self.device)\n","                target = target.to(self.device)\n","                src = data[:, 0, :]\n","                segs = data[:, 1, :]\n","                mask = data[:, 2, :]\n","                output = self.model(src, mask, segs, target)\n","                loss = output.loss\n","                self.val_total_loss += loss\n","                target_lst.extend(target.tolist())\n","                pred_lst.extend(output.logits.argmax(dim=1).tolist())\n","            self.val_mean_loss = self.val_total_loss / len(dataloader)\n","            self.validation_score = accuracy_score(y_true=target_lst, y_pred=pred_lst)\n","            msg = f'Epoch {epoch_index}, Validation, loss: {self.val_mean_loss}, Score: {self.validation_score}'\n","            print(msg)\n","            self.logger.info(msg) if self.logger else print(msg)\n","\n","    def test_epoch(self, dataloader, epoch_index):\n","        \"\"\" 한 epoch에서 수행되는 검증 절차\n","\n","        Args:\n","            dataloader (`dataloader`)\n","            epoch_index (int)\n","        \"\"\"\n","        self.model.eval()\n","        pred_lst = []\n","        with torch.no_grad():\n","            for batch_index, (data) in enumerate(dataloader):\n","                data = data.to(self.device)\n","                src = data[:, 0, :]\n","                segs = data[:, 1, :]\n","                mask = data[:, 2, :]\n","                output = self.model(src, mask, segs)\n","                pred_lst.extend(output.logits.argmax(dim=1).tolist())\n","                \n","                if batch_index % 100 == 0:\n","                    print(f'Prediction: {batch_index} batch completed')\n","        return pred_lst"],"id":"52ee23ce-a7de-447e-b4a4-eb892715ecd7","execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"collapsed":true,"id":"853b366d-2709-4eb8-915c-762134aa80b2","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["afc228ab11ad49d39dbe51cfcd2ef117","07ff7aa4466049ed8c91c57e04ea49bc","755eaf22fd5748b1a0a659c9eed36813","5323831ef982448ab450282da9669ecf","f7187e5aabcb41b0b833439b996ba814","ab19d08ae72b42eeb11b1c50e26b8bc3","327db6a5c4984109b5eaf50d2c0d4420","1fbb90ff64064fe6ad0d745f974ccc70","f9e5884d2abf49759711f9f52d005999","cf03ac82702f4e07845d83b0970a1c21","d65c9f79c1db44a688c4a0a32f61d09f"]},"executionInfo":{"status":"ok","timestamp":1630139415236,"user_tz":-540,"elapsed":17662,"user":{"displayName":"한주형","photoUrl":"","userId":"08880882316688407325"}},"outputId":"ac2597f7-1a65-4613-940a-4747d6e02b20"},"source":["# Load Model\n","model = IntentClassifier().to(device)\n","\n","# Save Initial Model\n","# torch.save({'model':model.state_dict()}, os.path.join(RESULT_DIR, 'initial.pt'))\n","\n","print('===== Review Model Architecture =====')\n","print(f'{model} \\n')\n","\n","# Set optimizer, scheduler, loss function, metric function\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","loss_fn = nn.CrossEntropyLoss()\n","scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e5, max_lr=0.0001, epochs=EPOCHS, steps_per_epoch=len(train_dataloader))\n","\n","# Set metrics\n","metric_fn = accuracy_score\n","\n","# Set trainer\n","trainer = Trainer(model, device, loss_fn, metric_fn ,optimizer, scheduler, logger=system_logger)\n","\n","# Set earlystopper\n","early_stopper = LossEarlyStopper(patience=EARLY_STOPPING_PATIENCE, verbose=True, logger=system_logger)"],"id":"853b366d-2709-4eb8-915c-762134aa80b2","execution_count":17,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"afc228ab11ad49d39dbe51cfcd2ef117","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/498M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","text":["===== Review Model Architecture =====\n","IntentClassifier(\n","  (model): BertForSequenceClassification(\n","    (bert): BertModel(\n","      (embeddings): BertEmbeddings(\n","        (word_embeddings): Embedding(50135, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): BertEncoder(\n","        (layer): ModuleList(\n","          (0): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (1): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (2): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (3): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (4): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (5): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (6): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (7): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (8): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (9): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (10): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (11): BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (classifier): Linear(in_features=768, out_features=118, bias=True)\n","  )\n","  (fc): Linear(in_features=768, out_features=6, bias=True)\n",") \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5d9ea277-b660-49f4-a715-9cd8c2447135","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1e954cd1-e837-44f5-bce4-a724efa6e009"},"source":["criterion = 0\n","# Train\n","for epoch_index in range(EPOCHS):\n","    trainer.train_epoch(train_dataloader, epoch_index=epoch_index)\n","    trainer.validate_epoch(validation_dataloader, epoch_index=epoch_index)\n","\n","    # early_stopping check\n","    early_stopper.check_early_stopping(loss=trainer.val_mean_loss)\n","\n","    if early_stopper.stop:\n","        print('Early stopped')\n","        break\n","\n","    if trainer.validation_score > criterion:\n","        criterion = trainer.validation_score\n","        check_point = {\n","            'model': model.state_dict(),\n","            'optimizer': optimizer.state_dict(),\n","            'scheduler': scheduler.state_dict()\n","        }\n","        torch.save(check_point, os.path.join(RESULT_DIR, 'best.pt'))"],"id":"5d9ea277-b660-49f4-a715-9cd8c2447135","execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 0 train batch 0/40985: 0/40985 mean loss: 4.867671489715576 score: 0.0\n","Epoch 0 train batch 100/40985: 800/40985 mean loss: 4.581225872039795 score: 0.0\n","Epoch 0 train batch 200/40985: 1600/40985 mean loss: 4.722387313842773 score: 0.0\n","Epoch 0 train batch 300/40985: 2400/40985 mean loss: 4.769691467285156 score: 0.0\n","Epoch 0 train batch 400/40985: 3200/40985 mean loss: 4.675554275512695 score: 0.0\n","Epoch 0 train batch 500/40985: 4000/40985 mean loss: 4.775263786315918 score: 0.000499001996007984\n","Epoch 0 train batch 600/40985: 4800/40985 mean loss: 4.728079795837402 score: 0.0008319467554076539\n","Epoch 0 train batch 700/40985: 5600/40985 mean loss: 4.612095355987549 score: 0.000891583452211127\n","Epoch 0 train batch 800/40985: 6400/40985 mean loss: 4.711632251739502 score: 0.0012484394506866417\n","Epoch 0 train batch 900/40985: 7200/40985 mean loss: 4.885817050933838 score: 0.0015260821309655938\n","Epoch 0 train batch 1000/40985: 8000/40985 mean loss: 4.548514366149902 score: 0.002122877122877123\n","Epoch 0 train batch 1100/40985: 8800/40985 mean loss: 4.669989585876465 score: 0.005563124432334241\n","Epoch 0 train batch 1200/40985: 9600/40985 mean loss: 4.585418224334717 score: 0.008950874271440467\n","Epoch 0 train batch 1300/40985: 10400/40985 mean loss: 4.656398773193359 score: 0.015564950038431976\n","Epoch 0 train batch 1400/40985: 11200/40985 mean loss: 4.361164569854736 score: 0.025517487508922198\n","Epoch 0 train batch 1500/40985: 12000/40985 mean loss: 4.499320030212402 score: 0.03539307128580946\n","Epoch 0 train batch 1600/40985: 12800/40985 mean loss: 4.417968273162842 score: 0.044815740162398504\n","Epoch 0 train batch 1700/40985: 13600/40985 mean loss: 4.499207973480225 score: 0.05254262198706643\n","Epoch 0 train batch 1800/40985: 14400/40985 mean loss: 4.368360996246338 score: 0.060244308717379236\n","Epoch 0 train batch 1900/40985: 15200/40985 mean loss: 4.346385955810547 score: 0.06674118884797475\n","Epoch 0 train batch 2000/40985: 16000/40985 mean loss: 4.168448448181152 score: 0.07290104947526237\n","Epoch 0 train batch 2100/40985: 16800/40985 mean loss: 4.599766254425049 score: 0.07954545454545454\n","Epoch 0 train batch 2200/40985: 17600/40985 mean loss: 4.484283447265625 score: 0.08399591094956838\n","Epoch 0 train batch 2300/40985: 18400/40985 mean loss: 4.3934197425842285 score: 0.08860278139939157\n","Epoch 0 train batch 2400/40985: 19200/40985 mean loss: 3.7555735111236572 score: 0.09261765930862141\n","Epoch 0 train batch 2500/40985: 20000/40985 mean loss: 4.204119682312012 score: 0.09576169532187125\n","Epoch 0 train batch 2600/40985: 20800/40985 mean loss: 4.194869518280029 score: 0.09919261822376009\n","Epoch 0 train batch 2700/40985: 21600/40985 mean loss: 3.748826503753662 score: 0.10338763420955202\n","Epoch 0 train batch 2800/40985: 22400/40985 mean loss: 3.9311094284057617 score: 0.10719385933595145\n","Epoch 0 train batch 2900/40985: 23200/40985 mean loss: 4.099860668182373 score: 0.11004825922095829\n","Epoch 0 train batch 3000/40985: 24000/40985 mean loss: 4.185464859008789 score: 0.11250416527824059\n","Epoch 0 train batch 3100/40985: 24800/40985 mean loss: 3.9096779823303223 score: 0.11548693969687197\n","Epoch 0 train batch 3200/40985: 25600/40985 mean loss: 2.879511594772339 score: 0.11828334895345205\n","Epoch 0 train batch 3300/40985: 26400/40985 mean loss: 3.5978915691375732 score: 0.12064525901242047\n","Epoch 0 train batch 3400/40985: 27200/40985 mean loss: 3.4054064750671387 score: 0.1234563363716554\n","Epoch 0 train batch 3500/40985: 28000/40985 mean loss: 3.254223108291626 score: 0.12571408169094545\n","Epoch 0 train batch 3600/40985: 28800/40985 mean loss: 3.8978271484375 score: 0.12808941960566508\n","Epoch 0 train batch 3700/40985: 29600/40985 mean loss: 3.973396062850952 score: 0.1303026209132667\n","Epoch 0 train batch 3800/40985: 30400/40985 mean loss: 3.0424647331237793 score: 0.13226782425677452\n","Epoch 0 train batch 3900/40985: 31200/40985 mean loss: 3.595531940460205 score: 0.1335554985901051\n","Epoch 0 train batch 4000/40985: 32000/40985 mean loss: 4.032803058624268 score: 0.13527868032991752\n","Epoch 0 train batch 4100/40985: 32800/40985 mean loss: 4.165250778198242 score: 0.13670446232626188\n","Epoch 0 train batch 4200/40985: 33600/40985 mean loss: 3.967435121536255 score: 0.13785408236134253\n","Epoch 0 train batch 4300/40985: 34400/40985 mean loss: 3.234548330307007 score: 0.1398802604045571\n","Epoch 0 train batch 4400/40985: 35200/40985 mean loss: 3.713198661804199 score: 0.1413315155646444\n","Epoch 0 train batch 4500/40985: 36000/40985 mean loss: 4.149736404418945 score: 0.142940457676072\n","Epoch 0 train batch 4600/40985: 36800/40985 mean loss: 3.483600378036499 score: 0.144425124972832\n","Epoch 0 train batch 4700/40985: 37600/40985 mean loss: 3.057701587677002 score: 0.1456073175920017\n","Epoch 0 train batch 4800/40985: 38400/40985 mean loss: 3.31050443649292 score: 0.1466881899604249\n","Epoch 0 train batch 4900/40985: 39200/40985 mean loss: 3.2015533447265625 score: 0.14849010406039584\n","Epoch 0 train batch 5000/40985: 40000/40985 mean loss: 3.2818667888641357 score: 0.14949510097980404\n","Epoch 0 train batch 5100/40985: 40800/40985 mean loss: 3.805002212524414 score: 0.15095079396196825\n","Epoch 0 train batch 5200/40985: 41600/40985 mean loss: 3.417248249053955 score: 0.15280715247067872\n","Epoch 0 train batch 5300/40985: 42400/40985 mean loss: 3.9722659587860107 score: 0.15431050745142427\n","Epoch 0 train batch 5400/40985: 43200/40985 mean loss: 4.007481575012207 score: 0.15543417885576746\n","Epoch 0 train batch 5500/40985: 44000/40985 mean loss: 3.0595459938049316 score: 0.1569260134520996\n","Epoch 0 train batch 5600/40985: 44800/40985 mean loss: 3.350496768951416 score: 0.15773968934118907\n","Epoch 0 train batch 5700/40985: 45600/40985 mean loss: 3.4474363327026367 score: 0.15898526574285213\n","Epoch 0 train batch 5800/40985: 46400/40985 mean loss: 3.2224838733673096 score: 0.15986467850370625\n","Epoch 0 train batch 5900/40985: 47200/40985 mean loss: 2.970054864883423 score: 0.16086256566683613\n","Epoch 0 train batch 6000/40985: 48000/40985 mean loss: 3.7516913414001465 score: 0.16197300449925012\n","Epoch 0 train batch 6100/40985: 48800/40985 mean loss: 3.1056199073791504 score: 0.16341583346992297\n","Epoch 0 train batch 6200/40985: 49600/40985 mean loss: 3.171769618988037 score: 0.16481212707627801\n","Epoch 0 train batch 6300/40985: 50400/40985 mean loss: 3.3836429119110107 score: 0.16562847167116332\n","Epoch 0 train batch 6400/40985: 51200/40985 mean loss: 3.093658924102783 score: 0.16667317606623966\n","Epoch 0 train batch 6500/40985: 52000/40985 mean loss: 3.1181442737579346 score: 0.16824334717735734\n","Epoch 0 train batch 6600/40985: 52800/40985 mean loss: 2.982715129852295 score: 0.16921678533555523\n","Epoch 0 train batch 6700/40985: 53600/40985 mean loss: 4.120965003967285 score: 0.17064617221310252\n","Epoch 0 train batch 6800/40985: 54400/40985 mean loss: 3.857179641723633 score: 0.17184972798117923\n","Epoch 0 train batch 6900/40985: 55200/40985 mean loss: 3.405337333679199 score: 0.1727104767425011\n","Epoch 0 train batch 7000/40985: 56000/40985 mean loss: 3.593113899230957 score: 0.1733859448650193\n","Epoch 0 train batch 7100/40985: 56800/40985 mean loss: 3.7651491165161133 score: 0.17423602309533867\n","Epoch 0 train batch 7200/40985: 57600/40985 mean loss: 2.9802157878875732 score: 0.1755138175253437\n","Epoch 0 train batch 7300/40985: 58400/40985 mean loss: 4.0304951667785645 score: 0.17701342281879195\n","Epoch 0 train batch 7400/40985: 59200/40985 mean loss: 3.153301477432251 score: 0.1783542764491285\n","Epoch 0 train batch 7500/40985: 60000/40985 mean loss: 3.1878087520599365 score: 0.1801426476469804\n","Epoch 0 train batch 7600/40985: 60800/40985 mean loss: 3.7424874305725098 score: 0.1817030653861334\n","Epoch 0 train batch 7700/40985: 61600/40985 mean loss: 4.129549503326416 score: 0.18405077262693156\n","Epoch 0 train batch 7800/40985: 62400/40985 mean loss: 3.605900287628174 score: 0.18585758236123573\n","Epoch 0 train batch 7900/40985: 63200/40985 mean loss: 3.122060775756836 score: 0.18782432603467916\n","Epoch 0 train batch 8000/40985: 64000/40985 mean loss: 3.7820403575897217 score: 0.18908573928258968\n","Epoch 0 train batch 8100/40985: 64800/40985 mean loss: 3.064234972000122 score: 0.19048574250092581\n","Epoch 0 train batch 8200/40985: 65600/40985 mean loss: 3.900757312774658 score: 0.19226313864162908\n","Epoch 0 train batch 8300/40985: 66400/40985 mean loss: 2.0913400650024414 score: 0.19402782797253343\n","Epoch 0 train batch 8400/40985: 67200/40985 mean loss: 2.603240966796875 score: 0.1953338888227592\n","Epoch 0 train batch 8500/40985: 68000/40985 mean loss: 2.5111920833587646 score: 0.19666803905422892\n","Epoch 0 train batch 8600/40985: 68800/40985 mean loss: 2.5243895053863525 score: 0.19830542960120917\n","Epoch 0 train batch 8700/40985: 69600/40985 mean loss: 1.749981164932251 score: 0.20013504194920123\n","Epoch 0 train batch 8800/40985: 70400/40985 mean loss: 3.5565030574798584 score: 0.2013407567321895\n","Epoch 0 train batch 8900/40985: 71200/40985 mean loss: 3.2908694744110107 score: 0.2028985507246377\n","Epoch 0 train batch 9000/40985: 72000/40985 mean loss: 1.9857052564620972 score: 0.20429674480613266\n","Epoch 0 train batch 9100/40985: 72800/40985 mean loss: 1.9211660623550415 score: 0.2056779474782991\n","Epoch 0 train batch 9200/40985: 73600/40985 mean loss: 2.467212200164795 score: 0.20704271274861427\n","Epoch 0 train batch 9300/40985: 74400/40985 mean loss: 1.6891553401947021 score: 0.20824373723255563\n","Epoch 0 train batch 9400/40985: 75200/40985 mean loss: 2.1503512859344482 score: 0.20959206467397085\n","Epoch 0 train batch 9500/40985: 76000/40985 mean loss: 3.8084802627563477 score: 0.21093832228186507\n","Epoch 0 train batch 9600/40985: 76800/40985 mean loss: 3.6005821228027344 score: 0.21223049682324757\n","Epoch 0 train batch 9700/40985: 77600/40985 mean loss: 3.8846256732940674 score: 0.2135089166065354\n","Epoch 0 train batch 9800/40985: 78400/40985 mean loss: 3.8256447315216064 score: 0.21450617283950618\n","Epoch 0 train batch 9900/40985: 79200/40985 mean loss: 2.868900775909424 score: 0.21568528431471568\n","Epoch 0 train batch 10000/40985: 80000/40985 mean loss: 3.501150131225586 score: 0.21707829217078292\n","Epoch 0 train batch 10100/40985: 80800/40985 mean loss: 2.5210702419281006 score: 0.2183075933075933\n","Epoch 0 train batch 10200/40985: 81600/40985 mean loss: 4.149100303649902 score: 0.21945152436035684\n","Epoch 0 train batch 10300/40985: 82400/40985 mean loss: 3.4109978675842285 score: 0.22088874866517813\n","Epoch 0 train batch 10400/40985: 83200/40985 mean loss: 3.147940158843994 score: 0.22220219209691375\n","Epoch 0 train batch 10500/40985: 84000/40985 mean loss: 1.8837400674819946 score: 0.22350252356918388\n","Epoch 0 train batch 10600/40985: 84800/40985 mean loss: 1.9219454526901245 score: 0.2247547401188567\n","Epoch 0 train batch 10700/40985: 85600/40985 mean loss: 2.614675283432007 score: 0.22601859639286048\n","Epoch 0 train batch 10800/40985: 86400/40985 mean loss: 2.5035483837127686 score: 0.22739792611795204\n","Epoch 0 train batch 10900/40985: 87200/40985 mean loss: 2.8635330200195312 score: 0.2285799467938721\n","Epoch 0 train batch 11000/40985: 88000/40985 mean loss: 2.8241162300109863 score: 0.229558676483956\n","Epoch 0 train batch 11100/40985: 88800/40985 mean loss: 2.2427361011505127 score: 0.23077875867039005\n","Epoch 0 train batch 11200/40985: 89600/40985 mean loss: 2.729097843170166 score: 0.23213329167038657\n","Epoch 0 train batch 11300/40985: 90400/40985 mean loss: 2.1221675872802734 score: 0.23314308468277142\n","Epoch 0 train batch 11400/40985: 91200/40985 mean loss: 2.6353960037231445 score: 0.23395974037365144\n","Epoch 0 train batch 11500/40985: 92000/40985 mean loss: 2.1671879291534424 score: 0.23516433353621424\n","Epoch 0 train batch 11600/40985: 92800/40985 mean loss: 2.414443254470825 score: 0.23630505990862857\n","Epoch 0 train batch 11700/40985: 93600/40985 mean loss: 3.035297393798828 score: 0.2372446799418853\n","Epoch 0 train batch 11800/40985: 94400/40985 mean loss: 2.1834166049957275 score: 0.23858147614608932\n","Epoch 0 train batch 11900/40985: 95200/40985 mean loss: 3.593827247619629 score: 0.23964372741786405\n","Epoch 0 train batch 12000/40985: 96000/40985 mean loss: 3.7314796447753906 score: 0.2405632863928006\n","Epoch 0 train batch 12100/40985: 96800/40985 mean loss: 3.874908924102783 score: 0.24169490124783075\n","Epoch 0 train batch 12200/40985: 97600/40985 mean loss: 2.9562830924987793 score: 0.24276698631259733\n","Epoch 0 train batch 12300/40985: 98400/40985 mean loss: 2.7385001182556152 score: 0.24358791968132673\n","Epoch 0 train batch 12400/40985: 99200/40985 mean loss: 2.4292924404144287 score: 0.24455689057334085\n","Epoch 0 train batch 12500/40985: 100000/40985 mean loss: 1.9944148063659668 score: 0.24564034877209823\n","Epoch 0 train batch 12600/40985: 100800/40985 mean loss: 2.635709762573242 score: 0.24672645028172369\n","Epoch 0 train batch 12700/40985: 101600/40985 mean loss: 3.23994779586792 score: 0.2477462404535076\n","Epoch 0 train batch 12800/40985: 102400/40985 mean loss: 2.4247093200683594 score: 0.24884774626982267\n","Epoch 0 train batch 12900/40985: 103200/40985 mean loss: 1.2850462198257446 score: 0.24974808154406636\n","Epoch 0 train batch 13000/40985: 104000/40985 mean loss: 3.406285285949707 score: 0.25067302515191137\n","Epoch 0 train batch 13100/40985: 104800/40985 mean loss: 2.2504465579986572 score: 0.2517365086634608\n","Epoch 0 train batch 13200/40985: 105600/40985 mean loss: 1.5367083549499512 score: 0.25303007347928186\n","Epoch 0 train batch 13300/40985: 106400/40985 mean loss: 2.792695999145508 score: 0.25405984512442675\n","Epoch 0 train batch 13400/40985: 107200/40985 mean loss: 3.0849764347076416 score: 0.25509290351466307\n","Epoch 0 train batch 13500/40985: 108000/40985 mean loss: 1.7835454940795898 score: 0.256175468483816\n","Epoch 0 train batch 13600/40985: 108800/40985 mean loss: 2.5238873958587646 score: 0.2572421145504007\n","Epoch 0 train batch 13700/40985: 109600/40985 mean loss: 2.7974746227264404 score: 0.25820195606160135\n","Epoch 0 train batch 13800/40985: 110400/40985 mean loss: 2.6183547973632812 score: 0.2591026012607782\n","Epoch 0 train batch 13900/40985: 111200/40985 mean loss: 2.184349536895752 score: 0.2602690453924178\n","Epoch 0 train batch 14000/40985: 112000/40985 mean loss: 2.287747621536255 score: 0.26123134061852726\n","Epoch 0 train batch 14100/40985: 112800/40985 mean loss: 2.1549322605133057 score: 0.2623040919083753\n","Epoch 0 train batch 14200/40985: 113600/40985 mean loss: 3.529353141784668 score: 0.2632120977395958\n","Epoch 0 train batch 14300/40985: 114400/40985 mean loss: 3.3941760063171387 score: 0.2645007342143906\n","Epoch 0 train batch 14400/40985: 115200/40985 mean loss: 1.7790848016738892 score: 0.2653982362335949\n","Epoch 0 train batch 14500/40985: 116000/40985 mean loss: 2.7490153312683105 score: 0.26638680091028205\n","Epoch 0 train batch 14600/40985: 116800/40985 mean loss: 2.470343828201294 score: 0.2671906033833299\n","Epoch 0 train batch 14700/40985: 117600/40985 mean loss: 2.8146557807922363 score: 0.268179035439766\n","Epoch 0 train batch 14800/40985: 118400/40985 mean loss: 2.978546142578125 score: 0.2690781028308898\n","Epoch 0 train batch 14900/40985: 119200/40985 mean loss: 2.4629433155059814 score: 0.2699818804107107\n","Epoch 0 train batch 15000/40985: 120000/40985 mean loss: 2.0493524074554443 score: 0.270915272315179\n","Epoch 0 train batch 15100/40985: 120800/40985 mean loss: 1.9697564840316772 score: 0.2718363022316403\n","Epoch 0 train batch 15200/40985: 121600/40985 mean loss: 2.3717305660247803 score: 0.2727452141306493\n","Epoch 0 train batch 15300/40985: 122400/40985 mean loss: 2.300199508666992 score: 0.2739036664270309\n","Epoch 0 train batch 15400/40985: 123200/40985 mean loss: 2.1138312816619873 score: 0.2751038893578339\n","Epoch 0 train batch 15500/40985: 124000/40985 mean loss: 2.4985928535461426 score: 0.276143474614541\n","Epoch 0 train batch 15600/40985: 124800/40985 mean loss: 2.2403392791748047 score: 0.2770335234920838\n","Epoch 0 train batch 15700/40985: 125600/40985 mean loss: 1.9253323078155518 score: 0.2781112667982931\n","Epoch 0 train batch 15800/40985: 126400/40985 mean loss: 2.7743804454803467 score: 0.2790171508132397\n","Epoch 0 train batch 15900/40985: 127200/40985 mean loss: 2.1647133827209473 score: 0.2800452801710584\n","Epoch 0 train batch 16000/40985: 128000/40985 mean loss: 2.5753579139709473 score: 0.28119336291481783\n","Epoch 0 train batch 16100/40985: 128800/40985 mean loss: 1.7418674230575562 score: 0.28221073225265514\n","Epoch 0 train batch 16200/40985: 129600/40985 mean loss: 1.283876895904541 score: 0.28313838651935064\n","Epoch 0 train batch 16300/40985: 130400/40985 mean loss: 3.189208984375 score: 0.28422336053002883\n","Epoch 0 train batch 16400/40985: 131200/40985 mean loss: 1.1055021286010742 score: 0.2853332113895494\n","Epoch 0 train batch 16500/40985: 132000/40985 mean loss: 2.3028273582458496 score: 0.2862402278649779\n","Epoch 0 train batch 16600/40985: 132800/40985 mean loss: 1.9569311141967773 score: 0.28727938076019516\n","Epoch 0 train batch 16700/40985: 133600/40985 mean loss: 2.3924477100372314 score: 0.28845578109095266\n","Epoch 0 train batch 16800/40985: 134400/40985 mean loss: 1.8956124782562256 score: 0.2895140170227963\n","Epoch 0 train batch 16900/40985: 135200/40985 mean loss: 1.999573826789856 score: 0.29048577007277676\n","Epoch 0 train batch 17000/40985: 136000/40985 mean loss: 1.438835859298706 score: 0.2916813716840186\n","Epoch 0 train batch 17100/40985: 136800/40985 mean loss: 2.758373737335205 score: 0.29271680018712354\n","Epoch 0 train batch 17200/40985: 137600/40985 mean loss: 0.9032785892486572 score: 0.29372565548514623\n","Epoch 0 train batch 17300/40985: 138400/40985 mean loss: 1.4200609922409058 score: 0.2950046240101728\n","Epoch 0 train batch 17400/40985: 139200/40985 mean loss: 1.2847427129745483 score: 0.29581633239468996\n","Epoch 0 train batch 17500/40985: 140000/40985 mean loss: 1.842625617980957 score: 0.29671875892806127\n","Epoch 0 train batch 17600/40985: 140800/40985 mean loss: 1.6033835411071777 score: 0.2977032554968468\n","Epoch 0 train batch 17700/40985: 141600/40985 mean loss: 3.2969629764556885 score: 0.2988108016496243\n","Epoch 0 train batch 17800/40985: 142400/40985 mean loss: 1.357406735420227 score: 0.29991292624009885\n","Epoch 0 train batch 17900/40985: 143200/40985 mean loss: 0.9579419493675232 score: 0.3010516172280878\n","Epoch 0 train batch 18000/40985: 144000/40985 mean loss: 2.6059021949768066 score: 0.30210127215154714\n","Epoch 0 train batch 18100/40985: 144800/40985 mean loss: 2.3432717323303223 score: 0.30308408375227885\n","Epoch 0 train batch 18200/40985: 145600/40985 mean loss: 1.9401521682739258 score: 0.30402862480083515\n","Epoch 0 train batch 18300/40985: 146400/40985 mean loss: 2.08298397064209 score: 0.3051267690290148\n","Epoch 0 train batch 18400/40985: 147200/40985 mean loss: 1.9614187479019165 score: 0.3061993913374273\n","Epoch 0 train batch 18500/40985: 148000/40985 mean loss: 2.3660593032836914 score: 0.30705097021782607\n","Epoch 0 train batch 18600/40985: 148800/40985 mean loss: 1.374607801437378 score: 0.3079941938605451\n","Epoch 0 train batch 18700/40985: 149600/40985 mean loss: 1.5742573738098145 score: 0.3090142238382974\n","Epoch 0 train batch 18800/40985: 150400/40985 mean loss: 2.0705325603485107 score: 0.3101430774958779\n","Epoch 0 train batch 18900/40985: 151200/40985 mean loss: 1.8425780534744263 score: 0.31122030580392573\n","Epoch 0 train batch 19000/40985: 152000/40985 mean loss: 2.191514730453491 score: 0.3122993526656492\n","Epoch 0 train batch 19100/40985: 152800/40985 mean loss: 1.3607903718948364 score: 0.3132034971990995\n","Epoch 0 train batch 19200/40985: 153600/40985 mean loss: 1.4782525300979614 score: 0.31435211707723554\n","Epoch 0 train batch 19300/40985: 154400/40985 mean loss: 2.182446241378784 score: 0.3153593078078856\n","Epoch 0 train batch 19400/40985: 155200/40985 mean loss: 2.695791482925415 score: 0.3163432297304263\n","Epoch 0 train batch 19500/40985: 156000/40985 mean loss: 2.318075180053711 score: 0.3173939797959079\n","Epoch 0 train batch 19600/40985: 156800/40985 mean loss: 1.3724322319030762 score: 0.3184276312433039\n","Epoch 0 train batch 19700/40985: 157600/40985 mean loss: 0.5571898818016052 score: 0.3194254098776712\n","Epoch 0 train batch 19800/40985: 158400/40985 mean loss: 2.3303446769714355 score: 0.3204383616989041\n","Epoch 0 train batch 19900/40985: 159200/40985 mean loss: 2.4417836666107178 score: 0.3213971659715592\n","Epoch 0 train batch 20000/40985: 160000/40985 mean loss: 2.0691752433776855 score: 0.3222713864306785\n","Epoch 0 train batch 20100/40985: 160800/40985 mean loss: 1.1828110218048096 score: 0.3232612805333068\n","Epoch 0 train batch 20200/40985: 161600/40985 mean loss: 1.944473385810852 score: 0.3242413741893966\n","Epoch 0 train batch 20300/40985: 162400/40985 mean loss: 2.181655168533325 score: 0.32517486823309194\n","Epoch 0 train batch 20400/40985: 163200/40985 mean loss: 2.2372305393218994 score: 0.32605632076858976\n","Epoch 0 train batch 20500/40985: 164000/40985 mean loss: 2.119236946105957 score: 0.32693527145017315\n","Epoch 0 train batch 20600/40985: 164800/40985 mean loss: 1.3365428447723389 score: 0.3277450123780399\n","Epoch 0 train batch 20700/40985: 165600/40985 mean loss: 1.1365936994552612 score: 0.32869185063523504\n","Epoch 0 train batch 20800/40985: 166400/40985 mean loss: 1.5212087631225586 score: 0.32964761309552426\n","Epoch 0 train batch 20900/40985: 167200/40985 mean loss: 1.4183831214904785 score: 0.33063609396679583\n","Epoch 0 train batch 21000/40985: 168000/40985 mean loss: 2.571235179901123 score: 0.33140088567211085\n","Epoch 0 train batch 21100/40985: 168800/40985 mean loss: 1.6341838836669922 score: 0.33220581962940143\n","Epoch 0 train batch 21200/40985: 169600/40985 mean loss: 1.748508095741272 score: 0.3331210791943776\n","Epoch 0 train batch 21300/40985: 170400/40985 mean loss: 1.8604786396026611 score: 0.3339690624853293\n","Epoch 0 train batch 21400/40985: 171200/40985 mean loss: 2.1029365062713623 score: 0.33499018737442177\n","Epoch 0 train batch 21500/40985: 172000/40985 mean loss: 1.873232126235962 score: 0.3359087949397702\n","Epoch 0 train batch 21600/40985: 172800/40985 mean loss: 2.505229949951172 score: 0.33671473542891533\n","Epoch 0 train batch 21700/40985: 173600/40985 mean loss: 2.230813503265381 score: 0.3376572508179347\n","Epoch 0 train batch 21800/40985: 174400/40985 mean loss: 1.5286164283752441 score: 0.33861405440117426\n","Epoch 0 train batch 21900/40985: 175200/40985 mean loss: 2.5933518409729004 score: 0.339510752933656\n","Epoch 0 train batch 22000/40985: 176000/40985 mean loss: 0.873934268951416 score: 0.3404333893913913\n","Epoch 0 train batch 22100/40985: 176800/40985 mean loss: 1.593917727470398 score: 0.34126849463825165\n","Epoch 0 train batch 22200/40985: 177600/40985 mean loss: 0.8060088157653809 score: 0.34219179316247017\n","Epoch 0 train batch 22300/40985: 178400/40985 mean loss: 2.314415216445923 score: 0.3430115241468992\n","Epoch 0 train batch 22400/40985: 179200/40985 mean loss: 1.5873674154281616 score: 0.3439913396723361\n","Epoch 0 train batch 22500/40985: 180000/40985 mean loss: 0.9858628511428833 score: 0.344862450557753\n","Epoch 0 train batch 22600/40985: 180800/40985 mean loss: 1.9809792041778564 score: 0.34572585283837\n","Epoch 0 train batch 22700/40985: 181600/40985 mean loss: 1.7579470872879028 score: 0.3466036738469671\n","Epoch 0 train batch 22800/40985: 182400/40985 mean loss: 2.416841506958008 score: 0.34754506381299066\n","Epoch 0 train batch 22900/40985: 183200/40985 mean loss: 1.8942548036575317 score: 0.3484727741146675\n","Epoch 0 train batch 23000/40985: 184000/40985 mean loss: 1.971280813217163 score: 0.349272857701839\n","Epoch 0 train batch 23100/40985: 184800/40985 mean loss: 2.179077386856079 score: 0.3501093026275919\n","Epoch 0 train batch 23200/40985: 185600/40985 mean loss: 2.3450257778167725 score: 0.3510139649153054\n","Epoch 0 train batch 23300/40985: 186400/40985 mean loss: 2.813051700592041 score: 0.35184112269859663\n","Epoch 0 train batch 23400/40985: 187200/40985 mean loss: 1.623693585395813 score: 0.35257040297423187\n","Epoch 0 train batch 23500/40985: 188000/40985 mean loss: 0.7194936871528625 score: 0.3534104931705034\n","Epoch 0 train batch 23600/40985: 188800/40985 mean loss: 1.938577651977539 score: 0.3541640184737935\n","Epoch 0 train batch 23700/40985: 189600/40985 mean loss: 1.9792845249176025 score: 0.35493755537741023\n","Epoch 0 train batch 23800/40985: 190400/40985 mean loss: 1.9237260818481445 score: 0.355788622326793\n","Epoch 0 train batch 23900/40985: 191200/40985 mean loss: 2.783797264099121 score: 0.35655411907451573\n","Epoch 0 train batch 24000/40985: 192000/40985 mean loss: 1.4969189167022705 score: 0.35748510478730056\n","Epoch 0 train batch 24100/40985: 192800/40985 mean loss: 1.7953481674194336 score: 0.35837724575743746\n","Epoch 0 train batch 24200/40985: 193600/40985 mean loss: 1.4831387996673584 score: 0.35911222676748894\n","Epoch 0 train batch 24300/40985: 194400/40985 mean loss: 2.268364429473877 score: 0.35987202172750093\n","Epoch 0 train batch 24400/40985: 195200/40985 mean loss: 2.1120214462280273 score: 0.36065632556042787\n","Epoch 0 train batch 24500/40985: 196000/40985 mean loss: 1.615082025527954 score: 0.36139341251377494\n","Epoch 0 train batch 24600/40985: 196800/40985 mean loss: 1.0544261932373047 score: 0.36214991260517865\n","Epoch 0 train batch 24700/40985: 197600/40985 mean loss: 2.977538824081421 score: 0.3629255900570827\n","Epoch 0 train batch 24800/40985: 198400/40985 mean loss: 2.1044020652770996 score: 0.3637050925365913\n","Epoch 0 train batch 24900/40985: 199200/40985 mean loss: 1.1818010807037354 score: 0.3643628769928918\n","Epoch 0 train batch 25000/40985: 200000/40985 mean loss: 1.1144888401031494 score: 0.3651053957841686\n","Epoch 0 train batch 25100/40985: 200800/40985 mean loss: 1.8940057754516602 score: 0.365732440938608\n","Epoch 0 train batch 25200/40985: 201600/40985 mean loss: 2.7641308307647705 score: 0.36651819372247135\n","Epoch 0 train batch 25300/40985: 202400/40985 mean loss: 2.1929750442504883 score: 0.36721868700841864\n","Epoch 0 train batch 25400/40985: 203200/40985 mean loss: 1.7375860214233398 score: 0.36805637573323885\n","Epoch 0 train batch 25500/40985: 204000/40985 mean loss: 1.800289511680603 score: 0.3688629857652641\n","Epoch 0 train batch 25600/40985: 204800/40985 mean loss: 1.50690495967865 score: 0.3696388812936995\n","Epoch 0 train batch 25700/40985: 205600/40985 mean loss: 1.2223429679870605 score: 0.3704816933193261\n","Epoch 0 train batch 25800/40985: 206400/40985 mean loss: 1.1248563528060913 score: 0.3711968528351614\n","Epoch 0 train batch 25900/40985: 207200/40985 mean loss: 1.3592392206192017 score: 0.3718871858229412\n","Epoch 0 train batch 26000/40985: 208000/40985 mean loss: 1.4184876680374146 score: 0.37267797392407986\n","Epoch 0 train batch 26100/40985: 208800/40985 mean loss: 2.610077381134033 score: 0.3734004444274166\n","Epoch 0 train batch 26200/40985: 209600/40985 mean loss: 1.2322063446044922 score: 0.37414602496087934\n","Epoch 0 train batch 26300/40985: 210400/40985 mean loss: 0.9080179929733276 score: 0.37473860309493934\n","Epoch 0 train batch 26400/40985: 211200/40985 mean loss: 1.2874603271484375 score: 0.3755113442672626\n","Epoch 0 train batch 26500/40985: 212000/40985 mean loss: 1.913025975227356 score: 0.37616505037545755\n","Epoch 0 train batch 26600/40985: 212800/40985 mean loss: 1.069157600402832 score: 0.3768984248712454\n","Epoch 0 train batch 26700/40985: 213600/40985 mean loss: 1.0957117080688477 score: 0.3776871652747088\n","Epoch 0 train batch 26800/40985: 214400/40985 mean loss: 1.5349395275115967 score: 0.37833942763329725\n","Epoch 0 train batch 26900/40985: 215200/40985 mean loss: 2.12534761428833 score: 0.3790007806401249\n","Epoch 0 train batch 27000/40985: 216000/40985 mean loss: 1.9322185516357422 score: 0.3796063108773749\n","Epoch 0 train batch 27100/40985: 216800/40985 mean loss: 1.0685298442840576 score: 0.3802996199402236\n","Epoch 0 train batch 27200/40985: 217600/40985 mean loss: 3.1060476303100586 score: 0.38108893055402376\n","Epoch 0 train batch 27300/40985: 218400/40985 mean loss: 2.0550785064697266 score: 0.3818495659499652\n","Epoch 0 train batch 27400/40985: 219200/40985 mean loss: 1.809989094734192 score: 0.3825225356738805\n","Epoch 0 train batch 27500/40985: 220000/40985 mean loss: 2.0433669090270996 score: 0.3831133413330424\n","Epoch 0 train batch 27600/40985: 220800/40985 mean loss: 1.2081621885299683 score: 0.3837043947683055\n","Epoch 0 train batch 27700/40985: 221600/40985 mean loss: 1.5530614852905273 score: 0.38436338038337964\n","Epoch 0 train batch 27800/40985: 222400/40985 mean loss: 0.5562008023262024 score: 0.3849771590949966\n","Epoch 0 train batch 27900/40985: 223200/40985 mean loss: 0.6119585633277893 score: 0.385658220135479\n","Epoch 0 train batch 28000/40985: 224000/40985 mean loss: 2.3475871086120605 score: 0.3863478090068212\n","Epoch 0 train batch 28100/40985: 224800/40985 mean loss: 1.6786342859268188 score: 0.38699690402476783\n","Epoch 0 train batch 28200/40985: 225600/40985 mean loss: 1.241363763809204 score: 0.3876591255629233\n","Epoch 0 train batch 28300/40985: 226400/40985 mean loss: 1.791025161743164 score: 0.38821508073919647\n","Epoch 0 train batch 28400/40985: 227200/40985 mean loss: 0.9612061381340027 score: 0.38897397978944404\n","Epoch 0 train batch 28500/40985: 228000/40985 mean loss: 1.1212152242660522 score: 0.38967930949791235\n","Epoch 0 train batch 28600/40985: 228800/40985 mean loss: 1.4214613437652588 score: 0.390410300339149\n","Epoch 0 train batch 28700/40985: 229600/40985 mean loss: 0.7893849015235901 score: 0.391101355353472\n","Epoch 0 train batch 28800/40985: 230400/40985 mean loss: 1.5716525316238403 score: 0.39174421027047673\n","Epoch 0 train batch 28900/40985: 231200/40985 mean loss: 1.28014075756073 score: 0.3924604684958998\n","Epoch 0 train batch 29000/40985: 232000/40985 mean loss: 1.7061984539031982 score: 0.39309420364815006\n","Epoch 0 train batch 29100/40985: 232800/40985 mean loss: 1.069919228553772 score: 0.3936763341465929\n","Epoch 0 train batch 29200/40985: 233600/40985 mean loss: 2.490297555923462 score: 0.39436577514468685\n","Epoch 0 train batch 29300/40985: 234400/40985 mean loss: 0.9197052717208862 score: 0.39491826217535236\n","Epoch 0 train batch 29400/40985: 235200/40985 mean loss: 2.262956380844116 score: 0.39560729226897046\n","Epoch 0 train batch 29500/40985: 236000/40985 mean loss: 0.9628996253013611 score: 0.3963170739974916\n","Epoch 0 train batch 29600/40985: 236800/40985 mean loss: 0.800896406173706 score: 0.396870038174386\n","Epoch 0 train batch 29700/40985: 237600/40985 mean loss: 1.5162806510925293 score: 0.39749503383724455\n","Epoch 0 train batch 29800/40985: 238400/40985 mean loss: 0.946123480796814 score: 0.39827103117345053\n","Epoch 0 train batch 29900/40985: 239200/40985 mean loss: 1.1899501085281372 score: 0.39897077020835425\n","Epoch 0 train batch 30000/40985: 240000/40985 mean loss: 1.7655436992645264 score: 0.3995783473884204\n","Epoch 0 train batch 30100/40985: 240800/40985 mean loss: 0.671141505241394 score: 0.4001984983887579\n","Epoch 0 train batch 30200/40985: 241600/40985 mean loss: 0.776229977607727 score: 0.4007938478858316\n","Epoch 0 train batch 30300/40985: 242400/40985 mean loss: 2.4906113147735596 score: 0.40147189861720733\n","Epoch 0 train batch 30400/40985: 243200/40985 mean loss: 0.4847644865512848 score: 0.402149600342094\n","Epoch 0 train batch 30500/40985: 244000/40985 mean loss: 0.6994709968566895 score: 0.40276138487262714\n","Epoch 0 train batch 30600/40985: 244800/40985 mean loss: 1.0686843395233154 score: 0.40340184961275777\n","Epoch 0 train batch 30700/40985: 245600/40985 mean loss: 2.2127413749694824 score: 0.40394449692192436\n","Epoch 0 train batch 30800/40985: 246400/40985 mean loss: 1.3212294578552246 score: 0.4045160871400279\n","Epoch 0 train batch 30900/40985: 247200/40985 mean loss: 1.4145041704177856 score: 0.4051122941005145\n","Epoch 0 train batch 31000/40985: 248000/40985 mean loss: 1.1990028619766235 score: 0.4056643334085997\n","Epoch 0 train batch 31100/40985: 248800/40985 mean loss: 0.8456152677536011 score: 0.40622488022893155\n","Epoch 0 train batch 31200/40985: 249600/40985 mean loss: 2.005293369293213 score: 0.4068579532707285\n","Epoch 0 train batch 31300/40985: 250400/40985 mean loss: 1.8724420070648193 score: 0.4074510399028785\n","Epoch 0 train batch 31400/40985: 251200/40985 mean loss: 1.144948124885559 score: 0.40804831056335783\n","Epoch 0 train batch 31500/40985: 252000/40985 mean loss: 0.6874263286590576 score: 0.4086417891495508\n","Epoch 0 train batch 31600/40985: 252800/40985 mean loss: 1.3687574863433838 score: 0.4092315116610234\n","Epoch 0 train batch 31700/40985: 253600/40985 mean loss: 2.3865115642547607 score: 0.409789911990158\n","Epoch 0 train batch 31800/40985: 254400/40985 mean loss: 0.9137932658195496 score: 0.4104076915820257\n","Epoch 0 train batch 31900/40985: 255200/40985 mean loss: 2.5600876808166504 score: 0.41100200620670196\n","Epoch 0 train batch 32000/40985: 256000/40985 mean loss: 1.266977071762085 score: 0.41158088809724697\n","Epoch 0 train batch 32100/40985: 256800/40985 mean loss: 1.1520224809646606 score: 0.4120899660446715\n","Epoch 0 train batch 32200/40985: 257600/40985 mean loss: 1.4829365015029907 score: 0.4125920002484395\n","Epoch 0 train batch 32300/40985: 258400/40985 mean loss: 1.087767481803894 score: 0.4131644531129067\n","Epoch 0 train batch 32400/40985: 259200/40985 mean loss: 1.2152882814407349 score: 0.41369479337057496\n","Epoch 0 train batch 32500/40985: 260000/40985 mean loss: 1.167038917541504 score: 0.4142487923448509\n","Epoch 0 train batch 32600/40985: 260800/40985 mean loss: 1.096751093864441 score: 0.4147380448452501\n","Epoch 0 train batch 32700/40985: 261600/40985 mean loss: 0.8484404683113098 score: 0.41527399773707224\n","Epoch 0 train batch 32800/40985: 262400/40985 mean loss: 1.9658067226409912 score: 0.41582954787963783\n","Epoch 0 train batch 32900/40985: 263200/40985 mean loss: 0.9156062602996826 score: 0.41634372815415943\n","Epoch 0 train batch 33000/40985: 264000/40985 mean loss: 1.2006779909133911 score: 0.4169608496712221\n","Epoch 0 train batch 33100/40985: 264800/40985 mean loss: 1.162374496459961 score: 0.41752137397661704\n","Epoch 0 train batch 33200/40985: 265600/40985 mean loss: 0.8011353611946106 score: 0.4180220475286889\n","Epoch 0 train batch 33300/40985: 266400/40985 mean loss: 1.6780911684036255 score: 0.4184934386354764\n","Epoch 0 train batch 33400/40985: 267200/40985 mean loss: 2.211297035217285 score: 0.4190256279752103\n","Epoch 0 train batch 33500/40985: 268000/40985 mean loss: 1.671262502670288 score: 0.4195844900152234\n","Epoch 0 train batch 33600/40985: 268800/40985 mean loss: 2.4855570793151855 score: 0.4200433022826702\n","Epoch 0 train batch 33700/40985: 269600/40985 mean loss: 1.1751227378845215 score: 0.4204919735319427\n","Epoch 0 train batch 33800/40985: 270400/40985 mean loss: 1.8591076135635376 score: 0.421019348539984\n","Epoch 0 train batch 33900/40985: 271200/40985 mean loss: 1.3219122886657715 score: 0.4215030530072859\n","Epoch 0 train batch 34000/40985: 272000/40985 mean loss: 1.3260859251022339 score: 0.4219949413252551\n","Epoch 0 train batch 34100/40985: 272800/40985 mean loss: 1.6795263290405273 score: 0.42255359080378874\n","Epoch 0 train batch 34200/40985: 273600/40985 mean loss: 0.9672070145606995 score: 0.422992017777258\n","Epoch 0 train batch 34300/40985: 274400/40985 mean loss: 1.5467268228530884 score: 0.42357730095332496\n","Epoch 0 train batch 34400/40985: 275200/40985 mean loss: 1.5367910861968994 score: 0.424122845266126\n","Epoch 0 train batch 34500/40985: 276000/40985 mean loss: 2.3272194862365723 score: 0.424498565258978\n","Epoch 0 train batch 34600/40985: 276800/40985 mean loss: 2.3578121662139893 score: 0.4250635819773995\n","Epoch 0 train batch 34700/40985: 277600/40985 mean loss: 1.3053849935531616 score: 0.42563254661248956\n","Epoch 0 train batch 34800/40985: 278400/40985 mean loss: 0.6003004312515259 score: 0.4261192207120485\n","Epoch 0 train batch 34900/40985: 279200/40985 mean loss: 1.7142252922058105 score: 0.4265529640984499\n","Epoch 0 train batch 35000/40985: 280000/40985 mean loss: 0.7541455030441284 score: 0.4270485128996314\n","Epoch 0 train batch 35100/40985: 280800/40985 mean loss: 1.5185191631317139 score: 0.4275519215976753\n","Epoch 0 train batch 35200/40985: 281600/40985 mean loss: 2.287911891937256 score: 0.4280205107809437\n","Epoch 0 train batch 35300/40985: 282400/40985 mean loss: 0.8923081159591675 score: 0.428497068071726\n","Epoch 0 train batch 35400/40985: 283200/40985 mean loss: 2.1278445720672607 score: 0.42893562328747775\n","Epoch 0 train batch 35500/40985: 284000/40985 mean loss: 2.099494457244873 score: 0.4294562125010563\n","Epoch 0 train batch 35600/40985: 284800/40985 mean loss: 1.4713386297225952 score: 0.4299773882756102\n","Epoch 0 train batch 35700/40985: 285600/40985 mean loss: 0.8391448259353638 score: 0.4304151144225652\n","Epoch 0 train batch 35800/40985: 286400/40985 mean loss: 0.6899839639663696 score: 0.43085388676293956\n","Epoch 0 train batch 35900/40985: 287200/40985 mean loss: 1.6000422239303589 score: 0.43136333249770203\n","Epoch 0 train batch 36000/40985: 288000/40985 mean loss: 1.6118733882904053 score: 0.4318664759312241\n","Epoch 0 train batch 36100/40985: 288800/40985 mean loss: 2.2127535343170166 score: 0.43237375695964103\n","Epoch 0 train batch 36200/40985: 289600/40985 mean loss: 1.7934658527374268 score: 0.43285751774812853\n","Epoch 0 train batch 36300/40985: 290400/40985 mean loss: 1.7999248504638672 score: 0.4333076223795488\n","Epoch 0 train batch 36400/40985: 291200/40985 mean loss: 1.8657596111297607 score: 0.4337037444026263\n","Epoch 0 train batch 36500/40985: 292000/40985 mean loss: 1.7960946559906006 score: 0.43423125393824824\n","Epoch 0 train batch 36600/40985: 292800/40985 mean loss: 2.6646616458892822 score: 0.43472172891451055\n","Epoch 0 train batch 36700/40985: 293600/40985 mean loss: 1.1440529823303223 score: 0.4352367782894199\n","Epoch 0 train batch 36800/40985: 294400/40985 mean loss: 0.6481744050979614 score: 0.43578299502730905\n","Epoch 0 train batch 36900/40985: 295200/40985 mean loss: 0.9779224991798401 score: 0.4362652773637571\n","Epoch 0 train batch 37000/40985: 296000/40985 mean loss: 1.3588005304336548 score: 0.4366942785330126\n","Epoch 0 train batch 37100/40985: 296800/40985 mean loss: 0.5369822382926941 score: 0.43721193498827526\n","Epoch 0 train batch 37200/40985: 297600/40985 mean loss: 2.0842087268829346 score: 0.4376596059245719\n","Epoch 0 train batch 37300/40985: 298400/40985 mean loss: 1.810021996498108 score: 0.4380914720784966\n","Epoch 0 train batch 37400/40985: 299200/40985 mean loss: 1.5051279067993164 score: 0.4384642121868399\n","Epoch 0 train batch 37500/40985: 300000/40985 mean loss: 1.3728142976760864 score: 0.4388282979120557\n","Epoch 0 train batch 37600/40985: 300800/40985 mean loss: 2.5649545192718506 score: 0.43925028589665166\n","Epoch 0 train batch 37700/40985: 301600/40985 mean loss: 0.8448455333709717 score: 0.4397230842683218\n","Epoch 0 train batch 37800/40985: 302400/40985 mean loss: 1.4150664806365967 score: 0.4400842570302373\n","Epoch 0 train batch 37900/40985: 303200/40985 mean loss: 1.1299079656600952 score: 0.4405754465581383\n","Epoch 0 train batch 38000/40985: 304000/40985 mean loss: 2.269893169403076 score: 0.44097194810662876\n","Epoch 0 train batch 38100/40985: 304800/40985 mean loss: 2.390841484069824 score: 0.44142870265872286\n","Epoch 0 train batch 38200/40985: 305600/40985 mean loss: 2.868662118911743 score: 0.44188306588832754\n","Epoch 0 train batch 38300/40985: 306400/40985 mean loss: 1.9658561944961548 score: 0.442338320148299\n","Epoch 0 train batch 38400/40985: 307200/40985 mean loss: 1.259297490119934 score: 0.442735866253483\n","Epoch 0 train batch 38500/40985: 308000/40985 mean loss: 1.9862980842590332 score: 0.4432255006363471\n","Epoch 0 train batch 38600/40985: 308800/40985 mean loss: 0.9556716680526733 score: 0.44361221211885704\n","Epoch 0 train batch 38700/40985: 309600/40985 mean loss: 1.4681168794631958 score: 0.4440453735045606\n","Epoch 0 train batch 38800/40985: 310400/40985 mean loss: 0.9972232580184937 score: 0.4444795237236154\n","Epoch 0 train batch 38900/40985: 311200/40985 mean loss: 2.532970666885376 score: 0.44490822858024215\n","Epoch 0 train batch 39000/40985: 312000/40985 mean loss: 0.5393365025520325 score: 0.44536358042101487\n","Epoch 0 train batch 39100/40985: 312800/40985 mean loss: 0.9434846639633179 score: 0.44575266617222065\n","Epoch 0 train batch 39200/40985: 313600/40985 mean loss: 1.395355463027954 score: 0.44619397464350397\n","Epoch 0 train batch 39300/40985: 314400/40985 mean loss: 1.3788471221923828 score: 0.44660759268212\n","Epoch 0 train batch 39400/40985: 315200/40985 mean loss: 0.9290745854377747 score: 0.4469715235653917\n","Epoch 0 train batch 39500/40985: 316000/40985 mean loss: 0.8896123766899109 score: 0.44737791448317765\n","Epoch 0 train batch 39600/40985: 316800/40985 mean loss: 2.744187116622925 score: 0.4477349056841999\n","Epoch 0 train batch 39700/40985: 317600/40985 mean loss: 3.025735855102539 score: 0.4481625147981159\n","Epoch 0 train batch 39800/40985: 318400/40985 mean loss: 1.4277631044387817 score: 0.44849689706288787\n","Epoch 0 train batch 39900/40985: 319200/40985 mean loss: 1.3455719947814941 score: 0.4489047893536503\n","Epoch 0 train batch 40000/40985: 320000/40985 mean loss: 1.2231016159057617 score: 0.4493731406714832\n","Epoch 0 train batch 40100/40985: 320800/40985 mean loss: 1.0728497505187988 score: 0.44977681354579685\n","Epoch 0 train batch 40200/40985: 321600/40985 mean loss: 0.9146283864974976 score: 0.45018158752269843\n","Epoch 0 train batch 40300/40985: 322400/40985 mean loss: 0.9219611287117004 score: 0.450581251085581\n","Epoch 0 train batch 40400/40985: 323200/40985 mean loss: 1.0456304550170898 score: 0.45094180837107994\n","Epoch 0 train batch 40500/40985: 324000/40985 mean loss: 1.1954461336135864 score: 0.4513715710723192\n","Epoch 0 train batch 40600/40985: 324800/40985 mean loss: 1.3097171783447266 score: 0.4517407206719046\n","Epoch 0 train batch 40700/40985: 325600/40985 mean loss: 1.175894021987915 score: 0.4521326257340115\n","Epoch 0 train batch 40800/40985: 326400/40985 mean loss: 1.8175243139266968 score: 0.45247359133354575\n","Epoch 0 train batch 40900/40985: 327200/40985 mean loss: 1.3207576274871826 score: 0.4529015183002861\n","Epoch 0, Train, loss: 2.256587266921997, Score: 0.4532611513534121\n","Epoch 0, Validation, loss: 1.4395017623901367, Score: 0.5911145220431483\n","Epoch 1 train batch 0/40985: 0/40985 mean loss: 1.0438261032104492 score: 0.625\n","Epoch 1 train batch 100/40985: 800/40985 mean loss: 0.9577778577804565 score: 0.620049504950495\n","Epoch 1 train batch 200/40985: 1600/40985 mean loss: 1.2847096920013428 score: 0.6094527363184079\n","Epoch 1 train batch 300/40985: 2400/40985 mean loss: 2.270766496658325 score: 0.622093023255814\n","Epoch 1 train batch 400/40985: 3200/40985 mean loss: 1.5638751983642578 score: 0.6221945137157108\n","Epoch 1 train batch 500/40985: 4000/40985 mean loss: 1.9863755702972412 score: 0.6272455089820359\n","Epoch 1 train batch 600/40985: 4800/40985 mean loss: 1.3265615701675415 score: 0.6191763727121464\n","Epoch 1 train batch 700/40985: 5600/40985 mean loss: 1.5603463649749756 score: 0.6189372325249644\n","Epoch 1 train batch 800/40985: 6400/40985 mean loss: 0.9167513847351074 score: 0.6192259675405742\n","Epoch 1 train batch 900/40985: 7200/40985 mean loss: 0.6228770017623901 score: 0.6202830188679245\n","Epoch 1 train batch 1000/40985: 8000/40985 mean loss: 0.8242295980453491 score: 0.6191308691308691\n","Epoch 1 train batch 1100/40985: 8800/40985 mean loss: 1.137971043586731 score: 0.6218210717529519\n","Epoch 1 train batch 1200/40985: 9600/40985 mean loss: 1.158523678779602 score: 0.6223980016652789\n","Epoch 1 train batch 1300/40985: 10400/40985 mean loss: 1.6269526481628418 score: 0.6228862413528056\n","Epoch 1 train batch 1400/40985: 11200/40985 mean loss: 0.9723504781723022 score: 0.6243754461099215\n","Epoch 1 train batch 1500/40985: 12000/40985 mean loss: 1.484973669052124 score: 0.6254996668887408\n","Epoch 1 train batch 1600/40985: 12800/40985 mean loss: 1.2454885244369507 score: 0.6260149906308557\n","Epoch 1 train batch 1700/40985: 13600/40985 mean loss: 0.6671161651611328 score: 0.6254409171075838\n","Epoch 1 train batch 1800/40985: 14400/40985 mean loss: 3.0986928939819336 score: 0.6250694058856191\n","Epoch 1 train batch 1900/40985: 15200/40985 mean loss: 1.328223705291748 score: 0.6229615991583377\n","Epoch 1 train batch 2000/40985: 16000/40985 mean loss: 1.2895987033843994 score: 0.6221264367816092\n","Epoch 1 train batch 2100/40985: 16800/40985 mean loss: 1.0084643363952637 score: 0.6233341266063779\n","Epoch 1 train batch 2200/40985: 17600/40985 mean loss: 1.0520089864730835 score: 0.6218764198091776\n","Epoch 1 train batch 2300/40985: 18400/40985 mean loss: 1.0644317865371704 score: 0.6203824424163408\n","Epoch 1 train batch 2400/40985: 19200/40985 mean loss: 0.24956846237182617 score: 0.6200020824656394\n","Epoch 1 train batch 2500/40985: 20000/40985 mean loss: 0.6341121196746826 score: 0.6203018792483007\n","Epoch 1 train batch 2600/40985: 20800/40985 mean loss: 1.0457162857055664 score: 0.6200499807766243\n","Epoch 1 train batch 2700/40985: 21600/40985 mean loss: 1.5715465545654297 score: 0.6199092928544984\n","Epoch 1 train batch 2800/40985: 22400/40985 mean loss: 1.303091049194336 score: 0.6199571581578008\n","Epoch 1 train batch 2900/40985: 23200/40985 mean loss: 2.336089849472046 score: 0.6203033436745949\n","Epoch 1 train batch 3000/40985: 24000/40985 mean loss: 1.4134925603866577 score: 0.6202515828057314\n","Epoch 1 train batch 3100/40985: 24800/40985 mean loss: 1.6714153289794922 score: 0.6193163495646565\n","Epoch 1 train batch 3200/40985: 25600/40985 mean loss: 1.952064871788025 score: 0.619337706966573\n","Epoch 1 train batch 3300/40985: 26400/40985 mean loss: 1.6293220520019531 score: 0.6187897606785823\n","Epoch 1 train batch 3400/40985: 27200/40985 mean loss: 1.8184564113616943 score: 0.619376653925316\n","Epoch 1 train batch 3500/40985: 28000/40985 mean loss: 1.3154304027557373 score: 0.6195372750642674\n","Epoch 1 train batch 3600/40985: 28800/40985 mean loss: 1.5847785472869873 score: 0.6191682865870591\n","Epoch 1 train batch 3700/40985: 29600/40985 mean loss: 1.7335736751556396 score: 0.6184139421777898\n","Epoch 1 train batch 3800/40985: 30400/40985 mean loss: 0.813241720199585 score: 0.6181268087345435\n","Epoch 1 train batch 3900/40985: 31200/40985 mean loss: 1.371286153793335 score: 0.6183670853627276\n","Epoch 1 train batch 4000/40985: 32000/40985 mean loss: 1.0059499740600586 score: 0.6191577105723569\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3UWr1W7JW6XN"},"source":["batch == 8 : Epoch 0 train batch 0/40985: 0/40985 mean loss: 4.864230632781982 score: 0.0"],"id":"3UWr1W7JW6XN"},{"cell_type":"markdown","metadata":{"id":"bc5862c2-54c2-4a65-8fc2-166f3ac1b171"},"source":["## Inference"],"id":"bc5862c2-54c2-4a65-8fc2-166f3ac1b171"},{"cell_type":"code","metadata":{"id":"d9871507-ce1c-4b01-8b74-9c3eaae4cd30"},"source":["TRAINED_MODEL_PATH = os.path.join(RESULT_DIR, 'best.pt')\n","\n","BATCH_SIZE = 32"],"id":"d9871507-ce1c-4b01-8b74-9c3eaae4cd30","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"37db4c1b-d9ec-46f9-b647-7e612b530940"},"source":["from torch.utils.data import Dataset\n","from transformers import AutoTokenizer\n","from itertools import chain\n","import sys\n","\n","class TestDataset(Dataset):\n","    def __init__(self, data_dir, mode):\n","        self.mode = mode\n","        self.data_dir = data_dir\n","        self.mode = mode\n","        self.data_dir = data_dir\n","        self.tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")\n","        \n","        self.intents = {0:'AS_날짜_요청' ,1:'AS_날짜_질문' ,2:'AS_방법_요청' ,3:'AS_방법_질문' ,4:'AS_비용_요청' ,5:'AS_비용_질문' ,\n","                        6:'AS_시간_질문' ,7:'AS_일반_질문' ,8:'결제_방식_질문' ,9:'결제_수단_질문' ,10:'결제_시기_질문' ,11:'결제_영수증_질문' ,\n","                        12:'결제_오류_질문' ,13:'결제_일반_질문' ,14:'결제_일반_확인' ,15:'결제_재결제_질문' ,16:'결제_추가_질문' ,\n","                        17:'결제_취소_질문' ,18:'결제_할인_질문' ,19:'교환|반품|환불_방법_요청' ,20:'교환|반품|환불_방법_질문' ,\n","                        21:'교환|반품|환불_방법_확인' ,22:'교환|반품|환불_비용_질문' ,23:'교환|반품|환불_시간_요청' ,\n","                        24:'교환|반품|환불_시간_질문' ,25:'교환|반품|환불_일반_요청' ,26:'교환|반품|환불_일반_질문' ,\n","                        27:'교환|반품|환불_일반_확인' ,28:'구매_예약_요청' ,29:'구매_예약_질문' ,30:'구매_제품_요청' ,31:'구매_제품_질문' ,\n","                        32:'매장_이용_요청' ,33:'매장_이용_질문' ,34:'매장_정보_질문' ,35:'멤버십_사용_질문' ,36:'멤버십_적립_질문' ,\n","                        37:'배송_날짜_요청' ,38:'배송_날짜_질문' ,39:'배송_날짜_확인' ,40:'배송_방법_요청' ,41:'배송_방법_질문' ,\n","                        42:'배송_방법_확인' ,43:'배송_비용_질문' ,44:'배송_오류_질문' ,45:'배송_오류_확인' ,46:'배송_일반_요청' ,\n","                        47:'배송_일반_질문' ,48:'배송_일반_확인' ,49:'배송_지역_요청' ,50:'배송_지역_질문' ,51:'배송_택배사_질문' ,\n","                        52:'부가서비스_날짜_요청' ,53:'부가서비스_날짜_질문' ,54:'부가서비스_방법_요청' ,55:'부가서비스_방법_질문' ,\n","                        56:'부가서비스_비용_요청' ,57:'부가서비스_비용_질문' ,58:'웹사이트_사용_질문' ,59:'웹사이트_오류_질문' ,\n","                        60:'제품_가격_비교' ,61:'제품_가격_요청' ,62:'제품_가격_질문' ,63:'제품_가격_확인' ,64:'제품_구성_요청' ,\n","                        65:'제품_구성_질문' ,66:'제품_구성_확인' ,67:'제품_날짜_질문' ,68:'제품_방법_요청' ,69:'제품_방법_질문' ,\n","                        70:'제품_방법_확인' ,71:'제품_불량_요청' ,72:'제품_불량_질문' ,73:'제품_불량_확인' ,74:'제품_소재_질문' ,\n","                        75:'제품_시용_요청' ,76:'제품_시용_질문' ,77:'제품_용도_질문' ,78:'제품_용도_확인' ,79:'제품_원산지_질문' ,\n","                        80:'제품_일반_비교' ,81:'제품_일반_요청' ,82:'제품_일반_질문' ,83:'제품_일반_확인' ,84:'제품_입고_요청' ,\n","                        85:'제품_입고_질문' ,86:'제품_재고_요청' ,87:'제품_재고_질문' ,88:'제품_재고_확인' ,89:'제품_정보_비교' ,\n","                        90:'제품_정보_요청' ,91:'제품_정보_질문' ,92:'제품_정보_확인' ,93:'제품_추천_비교' ,94:'제품_추천_요청' ,\n","                        95:'제품_추천_질문' ,96:'제품_추천_확인' ,97:'제품_커스텀_요청' ,98:'제품_커스텀_질문' ,99:'제품_품질_비교' ,\n","                        100:'제품_품질_요청' ,101:'제품_품질_질문' ,102:'제품_품질_확인' ,103:'제품_호환_질문' ,104:'제품_호환_확인' ,\n","                        105:'포장_방식_요청' ,106:'포장_방식_질문' ,107:'포장_비용_질문' ,108:'포장_일반_질문' ,109:'행사_기간_질문' ,\n","                        110:'행사_기간_확인' ,111:'행사_날짜_질문' ,112:'행사_유형_질문' ,113:'행사_유형_확인' ,114:'행사_일반_질문' ,\n","                        115:'행사_일반_확인' ,116:'행사_정보_요청' ,117:'행사_정보_질문' }\n","        \n","        # Load data\n","        self.inputs = self.data_loader(data_dir)\n","        self.conv_num = pd.read_csv(os.path.join(data_dir,'test','test.csv'))['conv_num']\n","\n","    def data_loader(self, path):\n","        print('Loading ' + self.mode + ' dataset..')\n","        # check if preprocessed data directory exists\n","        if not os.path.isdir(self.data_dir):\n","            print(f'!!! Cannot find {self.data_dir}... !!!')\n","            sys.exit()\n","\n","        if os.path.isfile(os.path.join(path, self.mode, self.mode + '_X.pt')):\n","            inputs = torch.load(os.path.join(path, self.mode, self.mode + '_X.pt'))\n","\n","        else:\n","            file_path = os.path.join(path, self.mode, self.mode + '.csv')\n","            df = pd.read_csv(file_path)\n","            df = df.dropna(axis=0, how='all')\n","            inputs = df[df.columns[1:]]\n","\n","            # Preprocessing\n","            inputs = self.preprocessing(inputs)\n","            # Save data\n","            torch.save(inputs ,os.path.join(path, self.mode, self.mode + '_X.pt'))\n","\n","        return inputs\n","\n","    def pad(self, data, pad_id, max_len):\n","        padded_data = list(map(lambda x : torch.cat([x, torch.tensor([pad_id] * (max_len - len(x)))]), data))\n","        return padded_data\n","\n","    def preprocessing(self, inputs):\n","        print('Preprocessing ' + self.mode + ' dataset..')\n","        #Encoding original\n","        src_tensor = []\n","        seg_tensor = []\n","        for i in range(len(inputs)):\n","            src_tensor.append(torch.tensor(list(chain.from_iterable([self.tokenizer.encode(inputs[col][i], add_special_tokens=True) \\\n","                                                                     for col in inputs.columns if inputs[col][i] == inputs[col][i]]))))\n","            clss = torch.cat([torch.where(src_tensor[i] == 2)[0], torch.tensor([len(src_tensor[i])])])\n","            seg_tensor.append(torch.tensor(list(chain.from_iterable( \\\n","                [[0] * (clss[i + 1] - clss[i]) if i % 2 == 0 else [1] * (clss[i + 1] - clss[i]) \\\n","                for i, val in enumerate(clss[:-1])]))))\n","\n","        #Padding\n","        max_encoding_len = max(list(map(lambda x: len(x), src_tensor)))\n","        assert max_encoding_len < 512, 'Encoding length is longer than maximum processing length.'\n","        src_tensor = self.pad(src_tensor, 0, max_encoding_len)\n","        seg_tensor = self.pad(seg_tensor, 0, max_encoding_len)\n","\n","        #Convert to list of tensor to 2d tensor\n","        src_tensor = torch.stack(src_tensor, dim=0)\n","        seg_tensor = torch.stack(seg_tensor, dim=0)\n","        mask_tensor = (~ (src_tensor == 0))\n","\n","        #Integrate the tensor {1st dimension : {src, seg, mask}, 2nd dim : {number of samples}, 3rd dim : {encoding dimension}}\n","        input_tensor = torch.cat([src_tensor.unsqueeze(dim=1) , seg_tensor.unsqueeze(dim=1), mask_tensor.unsqueeze(dim=1)], dim=1)\n","\n","        return input_tensor\n","\n","    def label_decoder(self, labels):\n","        try:\n","            labels = list(map(lambda x : self.intents[x], labels))\n","            return labels\n","        except:\n","            assert 'Invalid intent'\n","\n","    def __len__(self):\n","        return len(self.inputs)\n","\n","    def __getitem__(self, index):\n","        return self.inputs[index, :, :]"],"id":"37db4c1b-d9ec-46f9-b647-7e612b530940","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f1faec67-2927-4c75-b81e-a2685366a689"},"source":["# Load dataset & dataloader\n","test_dataset = TestDataset(data_dir=DATA_DIR, mode='test')\n","test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"],"id":"f1faec67-2927-4c75-b81e-a2685366a689","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"10891a02-8036-4f1d-b4d7-1c64a9e7a934"},"source":["# Load Model\n","model = IntentClassifier().to(device)\n","model.load_state_dict(torch.load(TRAINED_MODEL_PATH, map_location=device))\n","\"\"\" 현재 제공된 weight 파일은 model.state_dict()만 저장해서 위 코드로 실행하지만\n","    베이스라인의 학습 과정처럼 check_point를 저장하면 아래 코드로 실행 \"\"\"\n","# model.load_state_dict(torch.load(TRAINED_MODEL_PATH)['model'], map_location = device)\n","\n","# Set metrics & Loss function\n","metric_fn = accuracy_score\n","loss_fn = nn.CrossEntropyLoss()\n","\n","# Set trainer\n","trainer = Trainer(model, device, loss_fn, metric_fn)"],"id":"10891a02-8036-4f1d-b4d7-1c64a9e7a934","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fe2dbc9a-25d8-41e5-aaa7-c9ecbcdab62e"},"source":["# Predict\n","pred = []\n","pred = trainer.test_epoch(test_dataloader, epoch_index=0)\n","pred = test_dataset.label_decoder(pred)\n","print('decode completed--')\n","\n","# Save prediction\n","pred_df = pd.DataFrame()\n","pred_df['conv_num'] = test_dataset.conv_num\n","pred_df['intent'] = pred\n","\n","\n","pred_df.to_csv(os.path.join(os.path.join(RESULT_DIR,'02_retailer_pred.csv'), index=False))\n"],"id":"fe2dbc9a-25d8-41e5-aaa7-c9ecbcdab62e","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ycT0DpT14YhN"},"source":["Predict cell : 실행시간 14분 (GPU 사용시)"],"id":"ycT0DpT14YhN"}]}